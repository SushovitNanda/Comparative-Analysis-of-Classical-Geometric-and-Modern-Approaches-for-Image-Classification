# Comparative Analysis of Classical, Geometric, and
Modern Approaches for Image Classification and
Detection on MNIST

##  Project Overview

This project presents a comprehensive robustness study of various computer vision approaches on the MNIST handwritten digit classification task. The study evaluates multiple feature extraction methods and classification algorithms across different data augmentation scenarios to assess their robustness to common image transformations.

##  Objectives

1. **Robustness Evaluation**: Test classification performance under various image transformations (rotation, noise, scaling, occlusion)
2. **Feature Comparison**: Compare traditional handcrafted features (HOG, LBP, SIFT-BoW) with geometric features (Hough Transform, RANSAC) and deep learning approaches (ResNet-18, CLIP)
3. **Augmentation Analysis**: Evaluate the impact of different training augmentation strategies on model generalization
4. **Comprehensive Benchmarking**: Provide detailed performance metrics across 18 train-test combinations for each method

##  Project Structure

**Note**: This repository contains only the Jupyter notebooks and the project report. All results are generated by executing the notebooks.

```
CV Project/
â”œâ”€â”€ Dataset Augmentation.ipynb          # Data preprocessing and augmentation pipeline
â”œâ”€â”€ Traditional Feature Studies.ipynb  # HOG, LBP, SIFT-BoW feature extraction and classification
â”œâ”€â”€ Geometric Feature Studies.ipynb    # Hough Transform and RANSAC feature extraction
â”œâ”€â”€ RESNET18 Studies.ipynb             # Deep learning with ResNet-18
â”œâ”€â”€ CLIP Studies.ipynb                 # Zero-shot classification with CLIP
â””â”€â”€ Project_Report.pdf                 # Comprehensive project documentation
```

### Generated Results (Created When Running Notebooks)

When you execute the notebooks, the following result files will be generated:

- **Traditional Features** (`mnist_results_summary/`): HOG, LBP, SIFT-BoW results
- **Geometric Features** (`geometric_results/`): Hough Transform, RANSAC results
- **ResNet-18** (`resnet-18/`): Deep learning experiment results
- **CLIP** (`CLIP_results/mnist/`): Zero-shot classification results

## ðŸ”¬ Methodology

### Dataset Preparation

The MNIST dataset (60,000 training, 10,000 test images) is preprocessed and augmented as follows:

#### Training Sets (3 variants):
1. **original**: Clean training data (10,000 samples)
2. **mixed_augmented**: Equal parts of original + rotation + noise + scaling + occlusion (10,000 samples)
3. **combined_augmented**: All augmentations applied to every sample (10,000 samples)

#### Test Sets (6 variants):
1. **original**: Clean test data (2,000 samples)
2. **rotation_15**: 15Â° rotation applied
3. **noise**: Gaussian noise (Ïƒ=25)
4. **scaling_0.8**: 0.8x uniform scaling
5. **occlusion_25**: 25% random occlusion
6. **all_combined**: All augmentations combined

**Total Experiments**: 3 training sets Ã— 6 test sets = **18 combinations per method**

### Feature Extraction Methods

#### 1. Traditional Handcrafted Features

**HOG (Histogram of Oriented Gradients)**
- Parameters: 9 orientations, 8Ã—8 pixels per cell, 2Ã—2 cells per block
- Feature dimension: Variable based on image size
- Best Performance: **F1-Macro: 0.9696** (SVM_RBF, originalâ†’original)

**LBP (Local Binary Pattern)**
- Parameters: P=8, R=1, uniform patterns
- Feature dimension: 10 bins (histogram)
- Best Performance: **F1-Macro: 0.3602** (Random_Forest, originalâ†’original)

**SIFT-BoW (Scale-Invariant Feature Transform with Bag of Words)**
- SIFT descriptors extracted, clustered using K-Means (vocab sizes: 50, 100, 200)
- Feature dimension: Vocabulary size (histogram)
- Best Performance: **F1-Macro: 0.8149** (SVM_RBF, originalâ†’rotation_15)

#### 2. Geometric Features

**Hough Transform**
- Detects lines and circles in edge-detected images
- Features: Number of lines/circles, mean lengths/angles, edge density
- Best Performance: **F1-Macro: 0.4212** (SVM_RBF, originalâ†’original)

**RANSAC**
- Robust line and circle fitting
- Features: Inlier ratios for line and circle fits
- Best Performance: **F1-Macro: 0.2312** (SVM_RBF, originalâ†’occlusion_25)

**Combined (Hough + RANSAC)**
- Concatenated features from both methods
- Best Performance: **F1-Macro: 0.4601** (MLP, originalâ†’original)

#### 3. Deep Learning Approaches

**ResNet-18**
- Pretrained ImageNet weights, fine-tuned on MNIST
- Architecture: ResNet-18 with modified final layer (10 classes)
- Training: 20 epochs, early stopping (patience=4), Adam optimizer (lr=1e-4)
- Best Performance: **F1-Macro: 0.9892, Accuracy: 98.95%** (originalâ†’original)

**CLIP (Contrastive Language-Image Pre-training)**
- Model: `openai/clip-vit-large-patch14`
- Zero-shot classification using text prompts
- Prompt templates tested: basic, descriptive, context, minimal
- Best Performance: **F1-Macro: 0.7730, Accuracy: 76.5%** (minimal prompt, original test set)

### Classification Algorithms

All traditional and geometric feature methods are evaluated with:
- **SVM (RBF kernel)**
- **Logistic Regression**
- **Random Forest** (100 estimators)
- **XGBoost**
- **MLP** (Multi-Layer Perceptron: 100-50 hidden layers)
- **LightGBM**

##  Key Results Summary

### Overall Best Performances

| Method | Best F1-Macro | Best Accuracy | Best Configuration |
|--------|---------------|--------------|-------------------|
| **ResNet-18** | 0.9892 | 98.95% | originalâ†’original |
| **HOG + SVM** | 0.9696 | 97.00% | originalâ†’original |
| **SIFT-BoW + SVM** | 0.8149 | - | originalâ†’rotation_15 |
| **CLIP (minimal)** | 0.7730 | 76.50% | original (zero-shot) |
| **Combined Geometric + MLP** | 0.4601 | 47.80% | originalâ†’original |
| **LBP + Random Forest** | 0.3602 | - | originalâ†’original |

### Robustness Analysis

#### ResNet-18 Performance Across Test Sets:
- **Original**: 98.95% accuracy
- **Rotation 15Â°**: 97.55% accuracy
- **Scaling 0.8x**: 98.35% accuracy
- **Occlusion 25%**: 92.35% accuracy
- **Noise**: 16.55% accuracy (significant degradation)
- **All Combined**: 40.15% accuracy (severe degradation)

**Key Insight**: ResNet-18 trained on `combined_augmented` data shows better robustness to combined augmentations (96.45% accuracy) but lower performance on clean data (91.60%).

#### Traditional Features Robustness:
- **HOG**: Most robust among handcrafted features, maintains >90% F1 on most augmentations
- **SIFT-BoW**: Good performance on rotation (0.8149 F1) but struggles with noise
- **LBP**: Poor overall performance, sensitive to all transformations

#### CLIP Zero-Shot Performance:
- Best on **original** test set: 76.5% (minimal prompt)
- Best on **scaling**: 82.2% (minimal prompt)
- Struggles with **noise** and **all_combined**: <60% accuracy
- **Prompt Engineering Impact**: Minimal prompt (single word) outperforms descriptive prompts

##  Getting Started

### Repository Contents

This repository contains:
- **5 Jupyter Notebooks** (`.ipynb`) with complete experimental code
- **Project Report** (`Project_Report.pdf`) with comprehensive analysis

**Note**: Result files are NOT included in the repository. You must run the notebooks to generate all results.

### Prerequisites

```bash
# Python 3.8+
pip install numpy pandas matplotlib scikit-learn
pip install torch torchvision
pip install opencv-python pillow
pip install transformers
pip install xgboost lightgbm
pip install tqdm
```

### Installation

1. Clone or download this repository
2. Install required dependencies (see Prerequisites above)
3. Open the notebooks in Jupyter Lab/Notebook or Google Colab
4. Execute notebooks in order to generate results

### Running the Experiments

**Important**: Execute the notebooks in order to generate all results. The notebooks will create result directories and save detailed outputs.

#### 1. Dataset Preparation
```python
# Run: Dataset Augmentation.ipynb
# This will:
# - Download MNIST dataset
# - Resize images to 128Ã—128
# - Create all augmented training and test sets
# - Save to data/processed/
```

#### 2. Traditional Feature Extraction
```python
# Run: Traditional Feature Studies.ipynb
# This will:
# - Extract HOG, LBP, SIFT features
# - Train 6 classifiers on each feature type
# - Evaluate on all 18 train-test combinations
# - Print results to console and save to mnist_results_summary/
#
# Results Location: 
# - Console output: Classification reports for each experiment
# - Saved files: mnist_results_summary/GLOBAL_SUMMARY.txt
#                mnist_results_summary/HOG_Detailed_Summary.txt
#                mnist_results_summary/LBP_Detailed_Summary.txt
#                mnist_results_summary/SIFT_BoW_Detailed_Summary.txt
```

#### 3. Geometric Feature Extraction
```python
# Run: Geometric Feature Studies.ipynb
# This will:
# - Extract Hough Transform and RANSAC features
# - Train classifiers (SVM, Random Forest, XGBoost, MLP)
# - Generate visualizations
# - Print summary reports and save to geometric_results/
#
# Results Location:
# - Console output: Best results summary, performance metrics
# - Saved files: geometric_results/SUMMARY_REPORT.txt
#                geometric_results/Hough_Transform_classification_results.txt
#                geometric_results/RANSAC_classification_results.txt
#                geometric_results/Combined_Hough_RANSAC_classification_results.txt
#                geometric_results/visualizations/*.png
```

#### 4. Deep Learning (ResNet-18)
```python
# Run: RESNET18 Studies.ipynb
# This will:
# - Fine-tune ResNet-18 on each training set (original, mixed_augmented, combined_augmented)
# - Evaluate on all 6 test sets
# - Print training progress and test results
# - Save detailed reports to resnet-18/
#
# Results Location:
# - Console output: Training epochs, validation accuracy, test results
# - Saved files: resnet-18/resnet18_results.csv (summary)
#                resnet-18/mnist_train-*_test-*.txt (individual experiments)
```

#### 5. CLIP Zero-Shot
```python
# Run: CLIP Studies.ipynb
# This will:
# - Load CLIP model (openai/clip-vit-large-patch14)
# - Extract image embeddings
# - Test multiple prompt templates (basic, descriptive, context, minimal)
# - Print classification reports and save to CLIP_results/mnist/
#
# Results Location:
# - Console output: Overall results summary, prompt comparison
# - Saved files: CLIP_results/mnist/overall_summary.csv
#                CLIP_results/mnist/[test_set]/prompt_*/classification_report.txt
#                CLIP_results/mnist/[test_set]/prompt_*/predictions.csv
#                CLIP_results/mnist/[test_set]/prompt_*/confusion_matrix.csv
```

##  Results Interpretation

### Performance Rankings (by F1-Macro)

1. **ResNet-18** (0.9892) - Best overall, but sensitive to noise
2. **HOG + SVM** (0.9696) - Best handcrafted feature, robust to most transformations
3. **SIFT-BoW + SVM** (0.8149) - Good for rotation, poor for noise
4. **CLIP** (0.7730) - Zero-shot capability, no training required
5. **Combined Geometric** (0.4601) - Limited discriminative power
6. **LBP** (0.3602) - Poor performance overall

### Training Strategy Insights

- **Original Training**: Best for clean test data, poor robustness
- **Mixed Augmented**: Balanced performance, moderate robustness
- **Combined Augmented**: Best robustness to combined transformations, lower clean accuracy

### Augmentation Impact

- **Rotation**: Moderate impact, most methods handle well
- **Scaling**: Minimal impact, well-handled by all methods
- **Occlusion**: Significant impact, but manageable
- **Noise**: Severe impact, catastrophic for most methods
- **All Combined**: Extreme degradation, only combined_augmented training helps

##  Accessing Detailed Results

**All results are generated by executing the notebooks.** The notebooks print results to the console and save detailed files to respective directories.

### Traditional Features

**Notebook**: `Traditional Feature Studies.ipynb`

**Results Generated**:
- **Console Output**: Real-time classification reports, F1-scores, accuracy metrics for each experiment
- **Saved Files** (in `mnist_results_summary/`):
  - `GLOBAL_SUMMARY.txt`: Cross-method comparison across all feature types
  - `HOG_Detailed_Summary.txt`: Complete HOG results for all 18 train-test combinations
  - `LBP_Detailed_Summary.txt`: Complete LBP results
  - `SIFT_BoW_Detailed_Summary.txt`: Complete SIFT-BoW results

**Where to Find in Notebook**: Results are printed in the final execution cells and saved via the `save_feature_summary()` method.

### Geometric Features

**Notebook**: `Geometric Feature Studies.ipynb`

**Results Generated**:
- **Console Output**: Best results summary, performance rankings, visualization confirmations
- **Saved Files** (in `geometric_results/`):
  - `SUMMARY_REPORT.txt`: Overall geometric feature analysis with best configurations
  - `Hough_Transform_classification_results.txt`: All 18 combinations for Hough features
  - `RANSAC_classification_results.txt`: All 18 combinations for RANSAC features
  - `Combined_Hough_RANSAC_classification_results.txt`: Combined feature results
  - `visualizations/*.png`: Feature visualization plots

**Where to Find in Notebook**: Results are printed in "Step 5: Saving all results" section and final summary cells.

### ResNet-18

**Notebook**: `RESNET18 Studies.ipynb`

**Results Generated**:
- **Console Output**: Training progress, epoch-by-epoch metrics, test accuracy for each experiment
- **Saved Files** (in `resnet-18/`):
  - `resnet18_results.csv`: Summary table of all 18 train-test combinations
  - `mnist_train-{aug}_test-{aug}.txt`: Detailed reports for each experiment (18 files)

**Where to Find in Notebook**: Results are printed after each training session and consolidated in the final summary cell that creates `resnet18_results.csv`.

### CLIP Zero-Shot

**Notebook**: `CLIP Studies.ipynb`

**Results Generated**:
- **Console Output**: Overall results summary, prompt template comparison, per-test-set performance
- **Saved Files** (in `CLIP_results/mnist/`):
  - `overall_summary.csv`: Prompt template comparison across all test sets
  - `[test_set]/prompt_[template]/classification_report.txt`: Detailed classification reports
  - `[test_set]/prompt_[template]/predictions.csv`: Individual predictions
  - `[test_set]/prompt_[template]/confusion_matrix.csv`: Confusion matrices

**Where to Find in Notebook**: Results are printed in the final summary section showing "OVERALL RESULTS SUMMARY" and saved via the `save_results()` function.

### Quick Start to View Results

1. **Run the notebooks** in Jupyter/Colab to generate all result files
2. **Check console output** for immediate results and summaries
3. **Navigate to generated directories** for detailed saved files
4. **Refer to Project_Report.pdf** for comprehensive analysis and interpretation

##  Key Findings

1. **Deep Learning Dominance**: ResNet-18 achieves the highest accuracy (98.95%) on clean data
2. **HOG Superiority**: Among handcrafted features, HOG is most robust and achieves 96.96% F1
3. **Noise Sensitivity**: All methods struggle significantly with Gaussian noise
4. **Augmentation Trade-off**: Combined augmentation training improves robustness but reduces clean accuracy
5. **CLIP Limitations**: Zero-shot CLIP performs well (76.5%) but significantly below supervised methods
6. **Geometric Features**: Limited effectiveness for digit classification, better suited for shape detection

##  Important Notes

- **Results Not Included**: This repository contains only the code (notebooks) and documentation since the respective results directories are more than 2GB limit imposed by GitHub. All result files are generated when you execute the notebooks.
- **Execution Required**: To view detailed results, you must run the notebooks. Each notebook will:
  1. Print results to the console during execution
  2. Save detailed result files to respective directories
  3. Generate visualizations where applicable
- **Project Report**: See `Project_Report.pdf` for comprehensive analysis, methodology, and interpretation of results.
- **Execution Order**: While notebooks can be run independently, it's recommended to run `Dataset Augmentation.ipynb` first to generate the processed datasets.

---

**Last Updated**: Based on comprehensive experimental results across all methods and augmentation scenarios.

