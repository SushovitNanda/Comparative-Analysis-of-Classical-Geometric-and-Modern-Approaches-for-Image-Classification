{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205ac561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified CLIP Zero-Shot Image Classification for Jupyter Notebook\n",
    "\n",
    "Features:\n",
    "- Zero-shot classification with CLIP\n",
    "- Multiple prompt template experiments\n",
    "- Optional linear probe classifier\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION - EDIT THESE\n",
    "# =====================================================\n",
    "DATASET = 'mnist'  # 'mnist' or 'cifar10'\n",
    "TEST_ROOT = './data/processed'\n",
    "TRAIN_ROOT = './data/processed'  # Set to None to skip linear probe\n",
    "SAMPLE_N = 2000\n",
    "BATCH_SIZE = 128\n",
    "USE_LINEAR_PROBE = False\n",
    "TRAIN_TYPE = 'original'  # 'original', 'mixed_augmented', or 'combined_augmented'\n",
    "\n",
    "# =====================================================\n",
    "# Helper Functions\n",
    "# =====================================================\n",
    "\n",
    "def load_pickle_dataset(pkl_path):\n",
    "    \"\"\"Load pickle dataset and return list of (PIL.Image, int_label)\"\"\"\n",
    "    print(f\"Loading: {pkl_path}\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    items = []\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        if 'images' in data and 'labels' in data:\n",
    "            imgs = data['images']\n",
    "            labs = data['labels']\n",
    "            print(f\"Found {len(imgs)} images\")\n",
    "            for img, lab in zip(imgs, labs):\n",
    "                items.append((to_pil(img), int(lab)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dict format: {list(data.keys())}\")\n",
    "    \n",
    "    elif isinstance(data, (tuple, list)) and len(data) == 2:\n",
    "        imgs, labs = data[0], data[1]\n",
    "        print(f\"Found {len(imgs)} images\")\n",
    "        for img, lab in zip(imgs, labs):\n",
    "            items.append((to_pil(img), int(lab)))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pickle type: {type(data)}\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "def to_pil(img):\n",
    "    \"\"\"Convert numpy arrays to PIL.Image (RGB)\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "    if isinstance(img, np.ndarray):\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img]*3, axis=-1)\n",
    "        elif img.ndim == 3 and img.shape[-1] == 1:\n",
    "            img = np.stack([img.squeeze()]*3, axis=-1)\n",
    "        \n",
    "        if img.dtype != np.uint8:\n",
    "            if img.max() <= 1.0:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "            else:\n",
    "                img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return Image.fromarray(img).convert('RGB')\n",
    "    \n",
    "    raise ValueError(f\"Unsupported image type: {type(img)}\")\n",
    "\n",
    "\n",
    "class PickleImageDataset(Dataset):\n",
    "    def __init__(self, items, max_samples=None):\n",
    "        if max_samples and len(items) > max_samples:\n",
    "            items = items[:max_samples]\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, lab = self.items[idx]\n",
    "        return img, int(lab), idx\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(model, processor, dataloader, device):\n",
    "    \"\"\"Extract CLIP image embeddings\"\"\"\n",
    "    model.eval()\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "    all_idxs = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        imgs, labels, idxs = batch\n",
    "        \n",
    "        inputs = processor(images=list(imgs), return_tensors=\"pt\", padding=True).to(device)\n",
    "        image_feats = model.get_image_features(**inputs)\n",
    "        image_feats = image_feats / image_feats.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        all_feats.append(image_feats.cpu())\n",
    "        all_labels.extend([int(x) for x in labels])\n",
    "        all_idxs.extend([int(x) for x in idxs])\n",
    "    \n",
    "    if len(all_feats) == 0:\n",
    "        return np.zeros((0, model.config.projection_dim)), np.array([]), []\n",
    "    \n",
    "    all_feats = torch.cat(all_feats, dim=0).numpy()\n",
    "    return all_feats, np.array(all_labels), all_idxs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(model, processor, class_names, templates, device):\n",
    "    \"\"\"Build text embeddings from class names and templates\"\"\"\n",
    "    texts = []\n",
    "    for cname in class_names:\n",
    "        for t in templates:\n",
    "            texts.append(t.format(cname))\n",
    "    \n",
    "    all_text_feats = []\n",
    "    batch_size = 64\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = processor(text=batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "        tfeat = model.get_text_features(**inputs)\n",
    "        tfeat = tfeat / tfeat.norm(p=2, dim=-1, keepdim=True)\n",
    "        all_text_feats.append(tfeat.cpu())\n",
    "    \n",
    "    all_text_feats = torch.cat(all_text_feats, dim=0).numpy()\n",
    "    \n",
    "    # Average embeddings per class\n",
    "    num_templates = len(templates)\n",
    "    per_class = []\n",
    "    for i in range(0, all_text_feats.shape[0], num_templates):\n",
    "        per_class.append(all_text_feats[i:i+num_templates].mean(axis=0))\n",
    "    \n",
    "    per_class = np.vstack(per_class)\n",
    "    per_class = per_class / (np.linalg.norm(per_class, axis=1, keepdims=True) + 1e-12)\n",
    "    \n",
    "    return per_class\n",
    "\n",
    "\n",
    "def get_prompt_templates(dataset_name):\n",
    "    \"\"\"Define prompt template sets\"\"\"\n",
    "    \n",
    "    if dataset_name == 'mnist':\n",
    "        template_sets = {\n",
    "            'basic': [\n",
    "                \"a photo of the number {}.\",\n",
    "                \"the digit {}.\",\n",
    "            ],\n",
    "            'descriptive': [\n",
    "                \"a handwritten digit {}.\",\n",
    "                \"a photo of a handwritten number {}.\",\n",
    "                \"an image of the number {}.\",\n",
    "                \"a drawing of the digit {}.\",\n",
    "            ],\n",
    "            'context': [\n",
    "                \"a photo of a {} digit.\",\n",
    "                \"a black and white image of number {}.\",\n",
    "                \"a grayscale photo of the number {}.\",\n",
    "                \"handwriting showing the digit {}.\",\n",
    "            ],\n",
    "            'minimal': [\n",
    "                \"{}.\",\n",
    "            ],\n",
    "        }\n",
    "    else:  # CIFAR-10\n",
    "        template_sets = {\n",
    "            'basic': [\n",
    "                \"a photo of a {}.\",\n",
    "                \"an image of a {}.\",\n",
    "            ],\n",
    "            'descriptive': [\n",
    "                \"a photo of a {}.\",\n",
    "                \"a blurry photo of a {}.\",\n",
    "                \"a clear photo of a {}.\",\n",
    "                \"a bright photo of a {}.\",\n",
    "            ],\n",
    "            'context': [\n",
    "                \"a photo of a {} in the scene.\",\n",
    "                \"a picture of the {}.\",\n",
    "                \"an image showing a {}.\",\n",
    "            ],\n",
    "            'minimal': [\n",
    "                \"{}.\",\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    return template_sets\n",
    "\n",
    "\n",
    "def train_linear_probe(model, processor, device, train_pkl_path, batch_size=128, max_samples=10000):\n",
    "    \"\"\"Train linear probe classifier from training data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Linear Probe Classifier\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(train_pkl_path):\n",
    "        print(f\"Training data not found: {train_pkl_path}\")\n",
    "        return None\n",
    "    \n",
    "    items = load_pickle_dataset(train_pkl_path)\n",
    "    \n",
    "    if len(items) == 0:\n",
    "        print(\"No training data found\")\n",
    "        return None\n",
    "    \n",
    "    if max_samples and len(items) > max_samples:\n",
    "        print(f\"Using {max_samples} samples (out of {len(items)})\")\n",
    "        items = items[:max_samples]\n",
    "    \n",
    "    ds = PickleImageDataset(items)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                       collate_fn=lambda b: tuple(zip(*b)))\n",
    "    \n",
    "    print(\"Extracting training embeddings...\")\n",
    "    X_train, y_train, _ = extract_embeddings(model, processor, loader, device)\n",
    "    print(f\"Training embedding shape: {X_train.shape}\")\n",
    "    \n",
    "    print(\"Training logistic regression...\")\n",
    "    clf = LogisticRegression(max_iter=2000, multi_class='multinomial', \n",
    "                            solver='lbfgs', verbose=0, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    print(f\"Training accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "\n",
    "def run_classification(model, processor, device, test_pkl_path, class_names, \n",
    "                      template_sets, out_dir, batch_size=64, sample_n=2000, \n",
    "                      linear_probe_clf=None):\n",
    "    \"\"\"Run zero-shot and linear probe classification\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {os.path.basename(test_pkl_path)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    items = load_pickle_dataset(test_pkl_path)\n",
    "    if len(items) == 0:\n",
    "        print(\"No items found, skipping...\")\n",
    "        return\n",
    "    \n",
    "    if sample_n and len(items) > sample_n:\n",
    "        items = items[:sample_n]\n",
    "    print(f\"Using {len(items)} samples\")\n",
    "    \n",
    "    ds = PickleImageDataset(items)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, \n",
    "                       collate_fn=lambda b: tuple(zip(*b)))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X_test, y_test, idxs = extract_embeddings(model, processor, loader, device)\n",
    "    embed_time = time.time() - start_time\n",
    "    print(f\"Embedding time: {embed_time:.2f}s\")\n",
    "    print(f\"Embedding shape: {X_test.shape}\")\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Zero-shot with different prompts\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(\"Prompt Engineering Experiments\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for template_name, templates in template_sets.items():\n",
    "        print(f\"\\nTesting prompt set: {template_name}\")\n",
    "        print(f\"  Templates: {len(templates)}\")\n",
    "        \n",
    "        text_embeds = build_text_embeddings(model, processor, class_names, templates, device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        sims = X_test.dot(text_embeds.T)\n",
    "        y_pred = sims.argmax(axis=1)\n",
    "        pred_time = time.time() - start_time\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names, \n",
    "                                      zero_division=0, digits=4, output_dict=True)\n",
    "        \n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        prompt_dir = os.path.join(out_dir, f\"prompt_{template_name}\")\n",
    "        os.makedirs(prompt_dir, exist_ok=True)\n",
    "        \n",
    "        report_text = classification_report(y_test, y_pred, target_names=class_names,\n",
    "                                           zero_division=0, digits=4)\n",
    "        with open(os.path.join(prompt_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"Prompt Template Set: {template_name}\\n\")\n",
    "            f.write(f\"Number of Templates: {len(templates)}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.6f}\\n\\n\")\n",
    "            f.write(report_text)\n",
    "        \n",
    "        pd.DataFrame({\n",
    "            \"idx\": idxs,\n",
    "            \"true_label\": y_test.tolist(),\n",
    "            \"pred_label\": y_pred.tolist(),\n",
    "            \"correct\": (y_test == y_pred).astype(int).tolist()\n",
    "        }).to_csv(os.path.join(prompt_dir, \"predictions.csv\"), index=False)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "        cm_df.to_csv(os.path.join(prompt_dir, \"confusion_matrix.csv\"))\n",
    "        \n",
    "        results_summary.append({\n",
    "            'prompt_set': template_name,\n",
    "            'num_templates': len(templates),\n",
    "            'accuracy': acc,\n",
    "            'precision_macro': report['macro avg']['precision'],\n",
    "            'recall_macro': report['macro avg']['recall'],\n",
    "            'f1_macro': report['macro avg']['f1-score'],\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    summary_df.to_csv(os.path.join(out_dir, \"prompt_comparison.csv\"), index=False)\n",
    "    print(f\"\\nPrompt Comparison:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Linear probe (if available)\n",
    "    if linear_probe_clf is not None:\n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(\"Linear Probe Classification\")\n",
    "        print(f\"{'─'*60}\")\n",
    "        \n",
    "        y_pred_lp = linear_probe_clf.predict(X_test)\n",
    "        acc_lp = accuracy_score(y_test, y_pred_lp)\n",
    "        \n",
    "        print(f\"Accuracy: {acc_lp:.4f}\")\n",
    "        \n",
    "        lp_dir = os.path.join(out_dir, \"linear_probe\")\n",
    "        os.makedirs(lp_dir, exist_ok=True)\n",
    "        \n",
    "        report_lp = classification_report(y_test, y_pred_lp, target_names=class_names,\n",
    "                                         zero_division=0, digits=4)\n",
    "        with open(os.path.join(lp_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"Linear Probe Classification\\n\")\n",
    "            f.write(f\"Accuracy: {acc_lp:.6f}\\n\\n\")\n",
    "            f.write(report_lp)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLIP ZERO-SHOT CLASSIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Dataset: {DATASET.upper()}\")\n",
    "    print(f\"Samples per test: {SAMPLE_N}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Load CLIP model\n",
    "    model_name = \"openai/clip-vit-large-patch14\"\n",
    "    print(f\"\\nLoading CLIP model: {model_name}\")\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Define class names\n",
    "    if DATASET == 'mnist':\n",
    "        class_names = [str(i) for i in range(10)]\n",
    "    else:\n",
    "        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                      'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Get prompt templates\n",
    "    template_sets = get_prompt_templates(DATASET)\n",
    "    print(f\"\\nLoaded {len(template_sets)} prompt template sets\")\n",
    "    \n",
    "    # Define test files\n",
    "    test_folder = Path(TEST_ROOT) / f'{DATASET}_test'\n",
    "    test_files = {\n",
    "        'original': test_folder / 'original.pkl',\n",
    "        'noise': test_folder / 'noise.pkl',\n",
    "        'occlusion_25': test_folder / 'occlusion_25.pkl',\n",
    "        'rotation_15': test_folder / 'rotation_15.pkl',\n",
    "        'scaling_0.8': test_folder / 'scaling_0.8.pkl',\n",
    "        'all_combined': test_folder / 'all_combined.pkl',\n",
    "    }\n",
    "    \n",
    "    # Train linear probe if requested\n",
    "    linear_probe_clf = None\n",
    "    if USE_LINEAR_PROBE and TRAIN_ROOT:\n",
    "        train_folder = Path(TRAIN_ROOT) / f'{DATASET}_train'\n",
    "        train_file = train_folder / f'{TRAIN_TYPE}.pkl'\n",
    "        linear_probe_clf = train_linear_probe(model, processor, device, str(train_file), BATCH_SIZE)\n",
    "    \n",
    "    # Create output directory\n",
    "    base_out = Path(\"CLIP_results\") / DATASET\n",
    "    os.makedirs(base_out, exist_ok=True)\n",
    "    \n",
    "    # Process each test file\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING TEST SETS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for test_name, test_path in test_files.items():\n",
    "        if not test_path.exists():\n",
    "            print(f\"\\nTest file missing: {test_path}\")\n",
    "            continue\n",
    "        \n",
    "        out_dir = base_out / test_name\n",
    "        run_classification(\n",
    "            model, processor, device, str(test_path),\n",
    "            class_names, template_sets, str(out_dir),\n",
    "            batch_size=BATCH_SIZE, sample_n=SAMPLE_N,\n",
    "            linear_probe_clf=linear_probe_clf\n",
    "        )\n",
    "        \n",
    "        prompt_comp = pd.read_csv(out_dir / \"prompt_comparison.csv\")\n",
    "        prompt_comp['test_set'] = test_name\n",
    "        all_results.append(prompt_comp)\n",
    "    \n",
    "    # Create overall summary\n",
    "    if all_results:\n",
    "        overall_summary = pd.concat(all_results, ignore_index=True)\n",
    "        overall_summary.to_csv(base_out / \"overall_summary.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"OVERALL RESULTS SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(overall_summary.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Results saved to: {base_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d39413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLIP ZERO-SHOT CLASSIFICATION\n",
      "============================================================\n",
      "Dataset: MNIST\n",
      "Samples per test: 2000\n",
      "Batch size: 128\n",
      "============================================================\n",
      "Device: cuda\n",
      "\n",
      "Loading CLIP model: openai/clip-vit-large-patch14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daa821e11d94f0fa8ece36682d0c19a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  31%|###1      | 535M/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cas-bridge.xethub.hf.co/xet-bridge-us/621ffdc136468d709f17ea63/9046d5fe172d35ca65c0140b3d9c638d31b2714cc17049ee40fcf887ab0e076a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251030T161045Z&X-Amz-Expires=3600&X-Amz-Signature=3e74e961a3a0c1601fe696cfdf90d05f825599f3d171a00ffb36cf76593bc67a&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&x-id=GetObject&Expires=1761844245&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MTg0NDI0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82MjFmZmRjMTM2NDY4ZDcwOWYxN2VhNjMvOTA0NmQ1ZmUxNzJkMzVjYTY1YzAxNDBiM2Q5YzYzOGQzMWIyNzE0Y2MxNzA0OWVlNDBmY2Y4ODdhYjBlMDc2YSoifV19&Signature=A5rTJ4zeOTBr4ZXb2sCyUwndMyZ8pOlHZe5397I3JRTwZwNgeNgISbFz58rC%7EMQn6dPAI9dYqWQfxhnnxGolLTyQGH08o-7O8aBThrTM9UuYxQ9-HyZz2UksrROp%7ENZtUJCNzl3RohFKsd2Iq5OWq2KBjeFtZNFYe-Nlbeff3En0Vw2jPc4UjdopgI5JmwW89m8CiW9YVWB7nvPTayqQ0wfRCEwvfsdGJjDArIL3VrozakdHtae1pP0B3z6g%7Eno3EADLBIv34cWbdFzdXNEPh13LllJr2z2S1CEjfde87ZWcHMYdYrz18Hvx0iSXDlhGAFyDGfcx2SJlHdM7LoBxag__&Key-Pair-Id=K2L8F4GPSG1IFC: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c188265c66b44a48919bf93ad31496f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  99%|#########9| 1.70G/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd2c8c657c422495b02ef359fb7d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6873d34d124747e491bfbc060cdae946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0e675c298c24f86b5c8f753c2f0b2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666c22535a574847ab95071e53028172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1abf9576f014ab984c2010af798371d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59cba46369f44b39e75a8a377e405a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "Loaded 4 prompt template sets\n",
      "\n",
      "============================================================\n",
      "PROCESSING TEST SETS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: original.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\mnist_test\\original.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [01:33<00:00,  5.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 93.34s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.5740\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6050\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6670\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.7650\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2     0.574         0.805315      0.586243  0.606222\n",
      "descriptive              4     0.605         0.837253      0.617435  0.637133\n",
      "    context              4     0.667         0.830456      0.677376  0.686169\n",
      "    minimal              1     0.765         0.822337      0.761528  0.772981\n",
      "\n",
      "============================================================\n",
      "Processing: noise.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\mnist_test\\noise.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [01:30<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 90.80s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.6120\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6370\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6355\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.5865\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.6120         0.775248      0.621207  0.619895\n",
      "descriptive              4    0.6370         0.843786      0.642794  0.668670\n",
      "    context              4    0.6355         0.793156      0.643121  0.653565\n",
      "    minimal              1    0.5865         0.740603      0.588069  0.573733\n",
      "\n",
      "============================================================\n",
      "Processing: occlusion_25.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\mnist_test\\occlusion_25.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [01:33<00:00,  5.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 93.01s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.4940\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.5190\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 4\n",
      "  Accuracy: 0.5840\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.6770\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2     0.494         0.778157      0.503303  0.521672\n",
      "descriptive              4     0.519         0.799365      0.528160  0.546867\n",
      "    context              4     0.584         0.763119      0.591655  0.596160\n",
      "    minimal              1     0.677         0.744287      0.667781  0.678544\n",
      "\n",
      "============================================================\n",
      "Processing: rotation_15.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\mnist_test\\rotation_15.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [01:29<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 89.94s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.5605\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6035\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6305\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.6130\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.5605         0.810422      0.572405  0.590585\n",
      "descriptive              4    0.6035         0.831170      0.615640  0.631893\n",
      "    context              4    0.6305         0.812988      0.642468  0.660095\n",
      "    minimal              1    0.6130         0.707726      0.615866  0.604188\n",
      "\n",
      "============================================================\n",
      "Processing: scaling_0.8.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\mnist_test\\scaling_0.8.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [02:01<00:00,  7.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 121.82s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.6125\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6410\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 4\n",
      "  Accuracy: 0.7015\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.8220\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.6125         0.815664      0.624856  0.641650\n",
      "descriptive              4    0.6410         0.841102      0.653177  0.669379\n",
      "    context              4    0.7015         0.840821      0.712538  0.720389\n",
      "    minimal              1    0.8220         0.853111      0.819730  0.825157\n",
      "\n",
      "============================================================\n",
      "Processing: all_combined.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\mnist_test\\all_combined.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [01:57<00:00,  7.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 117.24s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.4890\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.4925\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 4\n",
      "  Accuracy: 0.5210\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.5125\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.4890         0.708652      0.495903  0.503450\n",
      "descriptive              4    0.4925         0.782224      0.501543  0.528810\n",
      "    context              4    0.5210         0.705658      0.527021  0.538912\n",
      "    minimal              1    0.5125         0.619658      0.513745  0.478350\n",
      "\n",
      "============================================================\n",
      "OVERALL RESULTS SUMMARY\n",
      "============================================================\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro     test_set\n",
      "      basic              2    0.5740         0.805315      0.586243  0.606222     original\n",
      "descriptive              4    0.6050         0.837253      0.617435  0.637133     original\n",
      "    context              4    0.6670         0.830456      0.677376  0.686169     original\n",
      "    minimal              1    0.7650         0.822337      0.761528  0.772981     original\n",
      "      basic              2    0.6120         0.775248      0.621207  0.619895        noise\n",
      "descriptive              4    0.6370         0.843786      0.642794  0.668670        noise\n",
      "    context              4    0.6355         0.793156      0.643121  0.653565        noise\n",
      "    minimal              1    0.5865         0.740603      0.588069  0.573733        noise\n",
      "      basic              2    0.4940         0.778157      0.503303  0.521672 occlusion_25\n",
      "descriptive              4    0.5190         0.799365      0.528160  0.546867 occlusion_25\n",
      "    context              4    0.5840         0.763119      0.591655  0.596160 occlusion_25\n",
      "    minimal              1    0.6770         0.744287      0.667781  0.678544 occlusion_25\n",
      "      basic              2    0.5605         0.810422      0.572405  0.590585  rotation_15\n",
      "descriptive              4    0.6035         0.831170      0.615640  0.631893  rotation_15\n",
      "    context              4    0.6305         0.812988      0.642468  0.660095  rotation_15\n",
      "    minimal              1    0.6130         0.707726      0.615866  0.604188  rotation_15\n",
      "      basic              2    0.6125         0.815664      0.624856  0.641650  scaling_0.8\n",
      "descriptive              4    0.6410         0.841102      0.653177  0.669379  scaling_0.8\n",
      "    context              4    0.7015         0.840821      0.712538  0.720389  scaling_0.8\n",
      "    minimal              1    0.8220         0.853111      0.819730  0.825157  scaling_0.8\n",
      "      basic              2    0.4890         0.708652      0.495903  0.503450 all_combined\n",
      "descriptive              4    0.4925         0.782224      0.501543  0.528810 all_combined\n",
      "    context              4    0.5210         0.705658      0.527021  0.538912 all_combined\n",
      "    minimal              1    0.5125         0.619658      0.513745  0.478350 all_combined\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Results saved to: CLIP_results\\mnist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0184af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simplified CLIP Zero-Shot Image Classification for Jupyter Notebook\n",
    "\n",
    "Features:\n",
    "- Zero-shot classification with CLIP\n",
    "- Multiple prompt template experiments\n",
    "- Optional linear probe classifier\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION - EDIT THESE\n",
    "# =====================================================\n",
    "DATASET = 'cifar10'  # 'mnist' or 'cifar10'\n",
    "TEST_ROOT = './data/processed'\n",
    "TRAIN_ROOT = './data/processed'  # Set to None to skip linear probe\n",
    "SAMPLE_N = 2000\n",
    "BATCH_SIZE = 128\n",
    "USE_LINEAR_PROBE = False\n",
    "TRAIN_TYPE = 'original'  # 'original', 'mixed_augmented', or 'combined_augmented'\n",
    "\n",
    "# =====================================================\n",
    "# Helper Functions\n",
    "# =====================================================\n",
    "\n",
    "def load_pickle_dataset(pkl_path):\n",
    "    \"\"\"Load pickle dataset and return list of (PIL.Image, int_label)\"\"\"\n",
    "    print(f\"Loading: {pkl_path}\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    items = []\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        if 'images' in data and 'labels' in data:\n",
    "            imgs = data['images']\n",
    "            labs = data['labels']\n",
    "            print(f\"Found {len(imgs)} images\")\n",
    "            for img, lab in zip(imgs, labs):\n",
    "                items.append((to_pil(img), int(lab)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dict format: {list(data.keys())}\")\n",
    "    \n",
    "    elif isinstance(data, (tuple, list)) and len(data) == 2:\n",
    "        imgs, labs = data[0], data[1]\n",
    "        print(f\"Found {len(imgs)} images\")\n",
    "        for img, lab in zip(imgs, labs):\n",
    "            items.append((to_pil(img), int(lab)))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pickle type: {type(data)}\")\n",
    "    \n",
    "    return items\n",
    "\n",
    "\n",
    "def to_pil(img):\n",
    "    \"\"\"Convert numpy arrays to PIL.Image (RGB)\"\"\"\n",
    "    if isinstance(img, Image.Image):\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "    if isinstance(img, np.ndarray):\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img]*3, axis=-1)\n",
    "        elif img.ndim == 3 and img.shape[-1] == 1:\n",
    "            img = np.stack([img.squeeze()]*3, axis=-1)\n",
    "        \n",
    "        if img.dtype != np.uint8:\n",
    "            if img.max() <= 1.0:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "            else:\n",
    "                img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        return Image.fromarray(img).convert('RGB')\n",
    "    \n",
    "    raise ValueError(f\"Unsupported image type: {type(img)}\")\n",
    "\n",
    "\n",
    "class PickleImageDataset(Dataset):\n",
    "    def __init__(self, items, max_samples=None):\n",
    "        if max_samples and len(items) > max_samples:\n",
    "            items = items[:max_samples]\n",
    "        self.items = items\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, lab = self.items[idx]\n",
    "        return img, int(lab), idx\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings(model, processor, dataloader, device):\n",
    "    \"\"\"Extract CLIP image embeddings\"\"\"\n",
    "    model.eval()\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "    all_idxs = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "        imgs, labels, idxs = batch\n",
    "        \n",
    "        inputs = processor(images=list(imgs), return_tensors=\"pt\", padding=True).to(device)\n",
    "        image_feats = model.get_image_features(**inputs)\n",
    "        image_feats = image_feats / image_feats.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        all_feats.append(image_feats.cpu())\n",
    "        all_labels.extend([int(x) for x in labels])\n",
    "        all_idxs.extend([int(x) for x in idxs])\n",
    "    \n",
    "    if len(all_feats) == 0:\n",
    "        return np.zeros((0, model.config.projection_dim)), np.array([]), []\n",
    "    \n",
    "    all_feats = torch.cat(all_feats, dim=0).numpy()\n",
    "    return all_feats, np.array(all_labels), all_idxs\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_text_embeddings(model, processor, class_names, templates, device):\n",
    "    \"\"\"Build text embeddings from class names and templates\"\"\"\n",
    "    texts = []\n",
    "    for cname in class_names:\n",
    "        for t in templates:\n",
    "            texts.append(t.format(cname))\n",
    "    \n",
    "    all_text_feats = []\n",
    "    batch_size = 64\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = processor(text=batch, return_tensors=\"pt\", padding=True).to(device)\n",
    "        tfeat = model.get_text_features(**inputs)\n",
    "        tfeat = tfeat / tfeat.norm(p=2, dim=-1, keepdim=True)\n",
    "        all_text_feats.append(tfeat.cpu())\n",
    "    \n",
    "    all_text_feats = torch.cat(all_text_feats, dim=0).numpy()\n",
    "    \n",
    "    # Average embeddings per class\n",
    "    num_templates = len(templates)\n",
    "    per_class = []\n",
    "    for i in range(0, all_text_feats.shape[0], num_templates):\n",
    "        per_class.append(all_text_feats[i:i+num_templates].mean(axis=0))\n",
    "    \n",
    "    per_class = np.vstack(per_class)\n",
    "    per_class = per_class / (np.linalg.norm(per_class, axis=1, keepdims=True) + 1e-12)\n",
    "    \n",
    "    return per_class\n",
    "\n",
    "\n",
    "def get_prompt_templates(dataset_name):\n",
    "    \"\"\"Define prompt template sets\"\"\"\n",
    "    \n",
    "    if dataset_name == 'mnist':\n",
    "        template_sets = {\n",
    "            'basic': [\n",
    "                \"a photo of the number {}.\",\n",
    "                \"the digit {}.\",\n",
    "            ],\n",
    "            'descriptive': [\n",
    "                \"a handwritten digit {}.\",\n",
    "                \"a photo of a handwritten number {}.\",\n",
    "                \"an image of the number {}.\",\n",
    "                \"a drawing of the digit {}.\",\n",
    "            ],\n",
    "            'context': [\n",
    "                \"a photo of a {} digit.\",\n",
    "                \"a black and white image of number {}.\",\n",
    "                \"a grayscale photo of the number {}.\",\n",
    "                \"handwriting showing the digit {}.\",\n",
    "            ],\n",
    "            'minimal': [\n",
    "                \"{}.\",\n",
    "            ],\n",
    "        }\n",
    "    else:  # CIFAR-10\n",
    "        template_sets = {\n",
    "            'basic': [\n",
    "                \"a photo of a {}.\",\n",
    "                \"an image of a {}.\",\n",
    "            ],\n",
    "            'descriptive': [\n",
    "                \"a photo of a {}.\",\n",
    "                \"a blurry photo of a {}.\",\n",
    "                \"a clear photo of a {}.\",\n",
    "                \"a bright photo of a {}.\",\n",
    "            ],\n",
    "            'context': [\n",
    "                \"a photo of a {} in the scene.\",\n",
    "                \"a picture of the {}.\",\n",
    "                \"an image showing a {}.\",\n",
    "            ],\n",
    "            'minimal': [\n",
    "                \"{}.\",\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    return template_sets\n",
    "\n",
    "\n",
    "def train_linear_probe(model, processor, device, train_pkl_path, batch_size=128, max_samples=10000):\n",
    "    \"\"\"Train linear probe classifier from training data\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training Linear Probe Classifier\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(train_pkl_path):\n",
    "        print(f\"Training data not found: {train_pkl_path}\")\n",
    "        return None\n",
    "    \n",
    "    items = load_pickle_dataset(train_pkl_path)\n",
    "    \n",
    "    if len(items) == 0:\n",
    "        print(\"No training data found\")\n",
    "        return None\n",
    "    \n",
    "    if max_samples and len(items) > max_samples:\n",
    "        print(f\"Using {max_samples} samples (out of {len(items)})\")\n",
    "        items = items[:max_samples]\n",
    "    \n",
    "    ds = PickleImageDataset(items)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                       collate_fn=lambda b: tuple(zip(*b)))\n",
    "    \n",
    "    print(\"Extracting training embeddings...\")\n",
    "    X_train, y_train, _ = extract_embeddings(model, processor, loader, device)\n",
    "    print(f\"Training embedding shape: {X_train.shape}\")\n",
    "    \n",
    "    print(\"Training logistic regression...\")\n",
    "    clf = LogisticRegression(max_iter=2000, multi_class='multinomial', \n",
    "                            solver='lbfgs', verbose=0, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    print(f\"Training accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    return clf\n",
    "\n",
    "\n",
    "def run_classification(model, processor, device, test_pkl_path, class_names, \n",
    "                      template_sets, out_dir, batch_size=64, sample_n=2000, \n",
    "                      linear_probe_clf=None):\n",
    "    \"\"\"Run zero-shot and linear probe classification\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {os.path.basename(test_pkl_path)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    items = load_pickle_dataset(test_pkl_path)\n",
    "    if len(items) == 0:\n",
    "        print(\"No items found, skipping...\")\n",
    "        return\n",
    "    \n",
    "    if sample_n and len(items) > sample_n:\n",
    "        items = items[:sample_n]\n",
    "    print(f\"Using {len(items)} samples\")\n",
    "    \n",
    "    ds = PickleImageDataset(items)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, \n",
    "                       collate_fn=lambda b: tuple(zip(*b)))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    X_test, y_test, idxs = extract_embeddings(model, processor, loader, device)\n",
    "    embed_time = time.time() - start_time\n",
    "    print(f\"Embedding time: {embed_time:.2f}s\")\n",
    "    print(f\"Embedding shape: {X_test.shape}\")\n",
    "    \n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Zero-shot with different prompts\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(\"Prompt Engineering Experiments\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    for template_name, templates in template_sets.items():\n",
    "        print(f\"\\nTesting prompt set: {template_name}\")\n",
    "        print(f\"  Templates: {len(templates)}\")\n",
    "        \n",
    "        text_embeds = build_text_embeddings(model, processor, class_names, templates, device)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        sims = X_test.dot(text_embeds.T)\n",
    "        y_pred = sims.argmax(axis=1)\n",
    "        pred_time = time.time() - start_time\n",
    "        \n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, target_names=class_names, \n",
    "                                      zero_division=0, digits=4, output_dict=True)\n",
    "        \n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "        \n",
    "        prompt_dir = os.path.join(out_dir, f\"prompt_{template_name}\")\n",
    "        os.makedirs(prompt_dir, exist_ok=True)\n",
    "        \n",
    "        report_text = classification_report(y_test, y_pred, target_names=class_names,\n",
    "                                           zero_division=0, digits=4)\n",
    "        with open(os.path.join(prompt_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"Prompt Template Set: {template_name}\\n\")\n",
    "            f.write(f\"Number of Templates: {len(templates)}\\n\")\n",
    "            f.write(f\"Accuracy: {acc:.6f}\\n\\n\")\n",
    "            f.write(report_text)\n",
    "        \n",
    "        pd.DataFrame({\n",
    "            \"idx\": idxs,\n",
    "            \"true_label\": y_test.tolist(),\n",
    "            \"pred_label\": y_pred.tolist(),\n",
    "            \"correct\": (y_test == y_pred).astype(int).tolist()\n",
    "        }).to_csv(os.path.join(prompt_dir, \"predictions.csv\"), index=False)\n",
    "        \n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "        cm_df.to_csv(os.path.join(prompt_dir, \"confusion_matrix.csv\"))\n",
    "        \n",
    "        results_summary.append({\n",
    "            'prompt_set': template_name,\n",
    "            'num_templates': len(templates),\n",
    "            'accuracy': acc,\n",
    "            'precision_macro': report['macro avg']['precision'],\n",
    "            'recall_macro': report['macro avg']['recall'],\n",
    "            'f1_macro': report['macro avg']['f1-score'],\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(results_summary)\n",
    "    summary_df.to_csv(os.path.join(out_dir, \"prompt_comparison.csv\"), index=False)\n",
    "    print(f\"\\nPrompt Comparison:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # Linear probe (if available)\n",
    "    if linear_probe_clf is not None:\n",
    "        print(f\"\\n{'─'*60}\")\n",
    "        print(\"Linear Probe Classification\")\n",
    "        print(f\"{'─'*60}\")\n",
    "        \n",
    "        y_pred_lp = linear_probe_clf.predict(X_test)\n",
    "        acc_lp = accuracy_score(y_test, y_pred_lp)\n",
    "        \n",
    "        print(f\"Accuracy: {acc_lp:.4f}\")\n",
    "        \n",
    "        lp_dir = os.path.join(out_dir, \"linear_probe\")\n",
    "        os.makedirs(lp_dir, exist_ok=True)\n",
    "        \n",
    "        report_lp = classification_report(y_test, y_pred_lp, target_names=class_names,\n",
    "                                         zero_division=0, digits=4)\n",
    "        with open(os.path.join(lp_dir, \"classification_report.txt\"), \"w\") as f:\n",
    "            f.write(f\"Linear Probe Classification\\n\")\n",
    "            f.write(f\"Accuracy: {acc_lp:.6f}\\n\\n\")\n",
    "            f.write(report_lp)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# MAIN EXECUTION\n",
    "# =====================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLIP ZERO-SHOT CLASSIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Dataset: {DATASET.upper()}\")\n",
    "    print(f\"Samples per test: {SAMPLE_N}\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    \n",
    "    # Load CLIP model\n",
    "    model_name = \"openai/clip-vit-large-patch14\"\n",
    "    print(f\"\\nLoading CLIP model: {model_name}\")\n",
    "    model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "    processor = CLIPProcessor.from_pretrained(model_name)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Define class names\n",
    "    if DATASET == 'mnist':\n",
    "        class_names = [str(i) for i in range(10)]\n",
    "    else:\n",
    "        class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                      'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    \n",
    "    # Get prompt templates\n",
    "    template_sets = get_prompt_templates(DATASET)\n",
    "    print(f\"\\nLoaded {len(template_sets)} prompt template sets\")\n",
    "    \n",
    "    # Define test files\n",
    "    test_folder = Path(TEST_ROOT) / f'{DATASET}_test'\n",
    "    test_files = {\n",
    "        'original': test_folder / 'original.pkl',\n",
    "        'noise': test_folder / 'noise.pkl',\n",
    "        'occlusion_25': test_folder / 'occlusion_25.pkl',\n",
    "        'rotation_15': test_folder / 'rotation_15.pkl',\n",
    "        'scaling_0.8': test_folder / 'scaling_0.8.pkl',\n",
    "        'all_combined': test_folder / 'all_combined.pkl',\n",
    "    }\n",
    "    \n",
    "    # Train linear probe if requested\n",
    "    linear_probe_clf = None\n",
    "    if USE_LINEAR_PROBE and TRAIN_ROOT:\n",
    "        train_folder = Path(TRAIN_ROOT) / f'{DATASET}_train'\n",
    "        train_file = train_folder / f'{TRAIN_TYPE}.pkl'\n",
    "        linear_probe_clf = train_linear_probe(model, processor, device, str(train_file), BATCH_SIZE)\n",
    "    \n",
    "    # Create output directory\n",
    "    base_out = Path(\"CLIP_results\") / DATASET\n",
    "    os.makedirs(base_out, exist_ok=True)\n",
    "    \n",
    "    # Process each test file\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING TEST SETS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for test_name, test_path in test_files.items():\n",
    "        if not test_path.exists():\n",
    "            print(f\"\\nTest file missing: {test_path}\")\n",
    "            continue\n",
    "        \n",
    "        out_dir = base_out / test_name\n",
    "        run_classification(\n",
    "            model, processor, device, str(test_path),\n",
    "            class_names, template_sets, str(out_dir),\n",
    "            batch_size=BATCH_SIZE, sample_n=SAMPLE_N,\n",
    "            linear_probe_clf=linear_probe_clf\n",
    "        )\n",
    "        \n",
    "        prompt_comp = pd.read_csv(out_dir / \"prompt_comparison.csv\")\n",
    "        prompt_comp['test_set'] = test_name\n",
    "        all_results.append(prompt_comp)\n",
    "    \n",
    "    # Create overall summary\n",
    "    if all_results:\n",
    "        overall_summary = pd.concat(all_results, ignore_index=True)\n",
    "        overall_summary.to_csv(base_out / \"overall_summary.csv\", index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"OVERALL RESULTS SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(overall_summary.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PROCESSING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Results saved to: {base_out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d554c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLIP ZERO-SHOT CLASSIFICATION\n",
      "============================================================\n",
      "Dataset: CIFAR10\n",
      "Samples per test: 2000\n",
      "Batch size: 128\n",
      "============================================================\n",
      "Device: cuda\n",
      "\n",
      "Loading CLIP model: openai/clip-vit-large-patch14\n",
      "Model loaded successfully\n",
      "\n",
      "Loaded 4 prompt template sets\n",
      "\n",
      "============================================================\n",
      "PROCESSING TEST SETS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: original.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\cifar10_test\\original.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [02:58<00:00, 11.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 178.98s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.9465\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.9425\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 3\n",
      "  Accuracy: 0.9475\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.8900\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.9465         0.947274      0.947724  0.946226\n",
      "descriptive              4    0.9425         0.944081      0.943663  0.942401\n",
      "    context              3    0.9475         0.948460      0.948535  0.947304\n",
      "    minimal              1    0.8900         0.900700      0.890074  0.887572\n",
      "\n",
      "============================================================\n",
      "Processing: noise.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\cifar10_test\\noise.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [03:31<00:00, 13.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 211.20s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.8310\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.8235\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 3\n",
      "  Accuracy: 0.8290\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.7055\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.8310         0.847364      0.833026  0.828274\n",
      "descriptive              4    0.8235         0.838988      0.825375  0.820677\n",
      "    context              3    0.8290         0.842592      0.831052  0.827136\n",
      "    minimal              1    0.7055         0.808597      0.708500  0.704236\n",
      "\n",
      "============================================================\n",
      "Processing: occlusion_25.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\cifar10_test\\occlusion_25.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [03:26<00:00, 12.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 206.69s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.9200\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.9175\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 3\n",
      "  Accuracy: 0.9245\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.8535\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.9200         0.922635      0.921314  0.919435\n",
      "descriptive              4    0.9175         0.920813      0.918869  0.917000\n",
      "    context              3    0.9245         0.926213      0.925535  0.924122\n",
      "    minimal              1    0.8535         0.881413      0.853735  0.851255\n",
      "\n",
      "============================================================\n",
      "Processing: rotation_15.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\cifar10_test\\rotation_15.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [03:20<00:00, 12.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 200.84s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.8920\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.8720\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 3\n",
      "  Accuracy: 0.8940\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.8115\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.8920         0.899606      0.893771  0.891304\n",
      "descriptive              4    0.8720         0.887419      0.874165  0.872003\n",
      "    context              3    0.8940         0.901683      0.895388  0.894165\n",
      "    minimal              1    0.8115         0.844298      0.812580  0.803963\n",
      "\n",
      "============================================================\n",
      "Processing: scaling_0.8.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\cifar10_test\\scaling_0.8.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [02:28<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 148.50s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.9520\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.9460\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 3\n",
      "  Accuracy: 0.9500\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.8940\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2     0.952         0.952398      0.952792  0.951858\n",
      "descriptive              4     0.946         0.947275      0.946901  0.945966\n",
      "    context              3     0.950         0.950468      0.950619  0.949767\n",
      "    minimal              1     0.894         0.903491      0.893861  0.890530\n",
      "\n",
      "============================================================\n",
      "Processing: all_combined.pkl\n",
      "============================================================\n",
      "Loading: data\\processed\\cifar10_test\\all_combined.pkl\n",
      "Found 10000 images\n",
      "Using 2000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding time: 27.20s\n",
      "Embedding shape: (2000, 768)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Prompt Engineering Experiments\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Testing prompt set: basic\n",
      "  Templates: 2\n",
      "  Accuracy: 0.6545\n",
      "\n",
      "Testing prompt set: descriptive\n",
      "  Templates: 4\n",
      "  Accuracy: 0.6455\n",
      "\n",
      "Testing prompt set: context\n",
      "  Templates: 3\n",
      "  Accuracy: 0.6275\n",
      "\n",
      "Testing prompt set: minimal\n",
      "  Templates: 1\n",
      "  Accuracy: 0.4880\n",
      "\n",
      "Prompt Comparison:\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro\n",
      "      basic              2    0.6545         0.716047      0.657432  0.650215\n",
      "descriptive              4    0.6455         0.709372      0.649093  0.638733\n",
      "    context              3    0.6275         0.709999      0.630754  0.632359\n",
      "    minimal              1    0.4880         0.745615      0.493167  0.508386\n",
      "\n",
      "============================================================\n",
      "OVERALL RESULTS SUMMARY\n",
      "============================================================\n",
      " prompt_set  num_templates  accuracy  precision_macro  recall_macro  f1_macro     test_set\n",
      "      basic              2    0.9465         0.947274      0.947724  0.946226     original\n",
      "descriptive              4    0.9425         0.944081      0.943663  0.942401     original\n",
      "    context              3    0.9475         0.948460      0.948535  0.947304     original\n",
      "    minimal              1    0.8900         0.900700      0.890074  0.887572     original\n",
      "      basic              2    0.8310         0.847364      0.833026  0.828274        noise\n",
      "descriptive              4    0.8235         0.838988      0.825375  0.820677        noise\n",
      "    context              3    0.8290         0.842592      0.831052  0.827136        noise\n",
      "    minimal              1    0.7055         0.808597      0.708500  0.704236        noise\n",
      "      basic              2    0.9200         0.922635      0.921314  0.919435 occlusion_25\n",
      "descriptive              4    0.9175         0.920813      0.918869  0.917000 occlusion_25\n",
      "    context              3    0.9245         0.926213      0.925535  0.924122 occlusion_25\n",
      "    minimal              1    0.8535         0.881413      0.853735  0.851255 occlusion_25\n",
      "      basic              2    0.8920         0.899606      0.893771  0.891304  rotation_15\n",
      "descriptive              4    0.8720         0.887419      0.874165  0.872003  rotation_15\n",
      "    context              3    0.8940         0.901683      0.895388  0.894165  rotation_15\n",
      "    minimal              1    0.8115         0.844298      0.812580  0.803963  rotation_15\n",
      "      basic              2    0.9520         0.952398      0.952792  0.951858  scaling_0.8\n",
      "descriptive              4    0.9460         0.947275      0.946901  0.945966  scaling_0.8\n",
      "    context              3    0.9500         0.950468      0.950619  0.949767  scaling_0.8\n",
      "    minimal              1    0.8940         0.903491      0.893861  0.890530  scaling_0.8\n",
      "      basic              2    0.6545         0.716047      0.657432  0.650215 all_combined\n",
      "descriptive              4    0.6455         0.709372      0.649093  0.638733 all_combined\n",
      "    context              3    0.6275         0.709999      0.630754  0.632359 all_combined\n",
      "    minimal              1    0.4880         0.745615      0.493167  0.508386 all_combined\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Results saved to: CLIP_results\\cifar10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb650fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
