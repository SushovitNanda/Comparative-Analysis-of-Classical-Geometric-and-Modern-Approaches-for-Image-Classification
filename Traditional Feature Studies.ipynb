{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e4b2ec",
   "metadata": {},
   "source": [
    "Feature Extraction (Step 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5d38e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from skimage.feature import hog, local_binary_pattern\n",
    "import cv2\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, processed_dir='./data/processed', feature_dir='./data/features', log_dir='./data/logs'):\n",
    "        \"\"\"\n",
    "        Step 3: Feature Extraction Only\n",
    "        Extract LBP, HOG, and SIFT features from preprocessed train and test files\n",
    "        \"\"\"\n",
    "        self.processed_dir = processed_dir\n",
    "        self.feature_dir = feature_dir\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        # Create directories\n",
    "        for directory in [feature_dir, log_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        # Setup logging\n",
    "        self._setup_logging()\n",
    "        \n",
    "        self.logger.info(\"Feature extractor initialized successfully\")\n",
    "        \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Setup comprehensive logging system\"\"\"\n",
    "        log_filename = f\"feature_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "        log_path = os.path.join(self.log_dir, log_filename)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_path),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def _load_data(self, pickle_path, sample_size=None):\n",
    "        \"\"\"Load data from pickle file and optionally sample\"\"\"\n",
    "        self.logger.info(f\"Loading data from: {pickle_path}\")\n",
    "        \n",
    "        with open(pickle_path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            if 'images' in data and 'labels' in data:\n",
    "                X, y = data['images'], data['labels']\n",
    "            elif 'X' in data and 'y' in data:\n",
    "                X, y = data['X'], data['y']\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported pickle structure\")\n",
    "        else:\n",
    "            X, y = data\n",
    "            \n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        \n",
    "        # Sample data if specified (10k for train, 2k for test)\n",
    "        if sample_size and len(X) > sample_size:\n",
    "            indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "            X, y = X[indices], y[indices]\n",
    "            self.logger.info(f\"Sampled {sample_size} examples from {len(X)} total\")\n",
    "        \n",
    "        self.logger.info(f\"Loaded {len(X)} images with {len(np.unique(y))} classes\")\n",
    "        return X, y\n",
    "    \n",
    "    def _ensure_gray(self, img):\n",
    "        \"\"\"Convert image to grayscale for feature extraction\"\"\"\n",
    "        if img.ndim == 2:\n",
    "            return img\n",
    "        if img.shape[-1] == 3:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        else:\n",
    "            gray = img[..., 0]\n",
    "        \n",
    "        # Normalize to 0-255 if needed\n",
    "        if gray.max() <= 1.0:\n",
    "            gray = (gray * 255).astype(np.uint8)\n",
    "        else:\n",
    "            gray = gray.astype(np.uint8)\n",
    "            \n",
    "        return gray\n",
    "    \n",
    "    def extract_lbp_features(self, images, P=8, R=1):\n",
    "        \"\"\"Extract Local Binary Pattern features\"\"\"\n",
    "        self.logger.info(\"Starting LBP feature extraction\")\n",
    "        \n",
    "        features = []\n",
    "        for i, img in enumerate(images):\n",
    "            gray = self._ensure_gray(img)\n",
    "            lbp = local_binary_pattern(gray, P, R, method='uniform')\n",
    "            hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, P + 3), range=(0, P + 2))\n",
    "            hist = hist.astype(np.float32)\n",
    "            hist /= (hist.sum() + 1e-6)  # Normalize\n",
    "            features.append(hist)\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                self.logger.info(f\"LBP: Processed {i + 1}/{len(images)} images\")\n",
    "        \n",
    "        self.logger.info(\"LBP feature extraction completed\")\n",
    "        return np.array(features)\n",
    "    \n",
    "    def extract_hog_features(self, images):\n",
    "        \"\"\"Extract HOG features\"\"\"\n",
    "        self.logger.info(\"Starting HOG feature extraction\")\n",
    "        \n",
    "        features = []\n",
    "        for i, img in enumerate(images):\n",
    "            gray = self._ensure_gray(img)\n",
    "            # Adjust parameters based on image size\n",
    "            pixels_per_cell = (8, 8)\n",
    "            if gray.shape[0] < 16 or gray.shape[1] < 16:\n",
    "                pixels_per_cell = (4, 4)\n",
    "                \n",
    "            hog_feat = hog(gray, orientations=9, pixels_per_cell=pixels_per_cell,\n",
    "                          cells_per_block=(2, 2), block_norm='L2-Hys', feature_vector=True)\n",
    "            features.append(hog_feat.astype(np.float32))\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                self.logger.info(f\"HOG: Processed {i + 1}/{len(images)} images\")\n",
    "        \n",
    "        self.logger.info(\"HOG feature extraction completed\")\n",
    "        return np.array(features)\n",
    "    \n",
    "    def extract_sift_features(self, images):\n",
    "        \"\"\"Extract SIFT descriptors\"\"\"\n",
    "        self.logger.info(\"Starting SIFT feature extraction\")\n",
    "        \n",
    "        sift = cv2.SIFT_create()\n",
    "        all_descriptors = []\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            gray = self._ensure_gray(img)\n",
    "            _, descriptors = sift.detectAndCompute(gray, None)\n",
    "            \n",
    "            if descriptors is not None:\n",
    "                all_descriptors.append(descriptors)\n",
    "            else:\n",
    "                # Create empty descriptor if no features found\n",
    "                all_descriptors.append(np.array([]).reshape(0, 128))\n",
    "            \n",
    "            if (i + 1) % 1000 == 0:\n",
    "                self.logger.info(f\"SIFT: Processed {i + 1}/{len(images)} images\")\n",
    "        \n",
    "        self.logger.info(\"SIFT feature extraction completed\")\n",
    "        return all_descriptors\n",
    "    \n",
    "    def save_features(self, features, labels, feature_type, dataset_name, set_type, augmentation_type):\n",
    "        \"\"\"Save extracted features to pickle files\"\"\"\n",
    "        save_dir = os.path.join(self.feature_dir, dataset_name, feature_type, set_type)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        filename = f\"{augmentation_type}.pkl\"\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        \n",
    "        # Save as dictionary for easy loading later\n",
    "        feature_data = {\n",
    "            'features': features,\n",
    "            'labels': labels,\n",
    "            'feature_type': feature_type,\n",
    "            'dataset_name': dataset_name,\n",
    "            'set_type': set_type,\n",
    "            'augmentation_type': augmentation_type,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(feature_data, f)\n",
    "        \n",
    "        self.logger.info(f\"Saved {feature_type} features to: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def extract_features_for_file(self, pickle_path, dataset_name, set_type, augmentation_type, \n",
    "                                 sample_size=None, feature_types=['LBP', 'HOG', 'SIFT']):\n",
    "        \"\"\"Extract all features for a single pickle file\"\"\"\n",
    "        self.logger.info(f\"Processing: {dataset_name} - {set_type} - {augmentation_type}\")\n",
    "        \n",
    "        # Load data (with sampling if specified)\n",
    "        X, y = self._load_data(pickle_path, sample_size)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for feature_type in feature_types:\n",
    "            self.logger.info(f\"Extracting {feature_type} features...\")\n",
    "            \n",
    "            try:\n",
    "                if feature_type == 'LBP':\n",
    "                    features = self.extract_lbp_features(X)\n",
    "                elif feature_type == 'HOG':\n",
    "                    features = self.extract_hog_features(X)\n",
    "                elif feature_type == 'SIFT':\n",
    "                    features = self.extract_sift_features(X)\n",
    "                \n",
    "                # Save features\n",
    "                save_path = self.save_features(features, y, feature_type, dataset_name, \n",
    "                                             set_type, augmentation_type)\n",
    "                \n",
    "                # Store feature info\n",
    "                if feature_type == 'SIFT':\n",
    "                    # For SIFT, we have list of variable-length descriptors\n",
    "                    shapes = [f.shape if len(f) > 0 else (0, 128) for f in features]\n",
    "                    results[feature_type] = {\n",
    "                        'num_descriptors': sum(len(f) for f in features),\n",
    "                        'shapes': shapes,\n",
    "                        'save_path': save_path\n",
    "                    }\n",
    "                else:\n",
    "                    # For LBP and HOG, we have fixed-length features\n",
    "                    results[feature_type] = {\n",
    "                        'features_shape': features.shape,\n",
    "                        'save_path': save_path\n",
    "                    }\n",
    "                \n",
    "                self.logger.info(f\"{feature_type} features extracted successfully\")\n",
    "                \n",
    "                # Cleanup\n",
    "                del features\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error extracting {feature_type} features: {str(e)}\")\n",
    "                results[feature_type] = {'error': str(e)}\n",
    "        \n",
    "        # Cleanup\n",
    "        del X, y\n",
    "        gc.collect()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_feature_extraction(self):\n",
    "        \"\"\"Run complete feature extraction for all datasets and files\"\"\"\n",
    "        self.logger.info(\"Starting feature extraction pipeline\")\n",
    "        \n",
    "        datasets = ['cifar10', 'mnist']\n",
    "        \n",
    "        # Train files - sample 10,000 from each\n",
    "        train_files = [\n",
    "            ('original.pkl', 'original'),\n",
    "            ('mixed_augmented.pkl', 'mixed_augmented'), \n",
    "            ('combined_augmented.pkl', 'combined_augmented')\n",
    "        ]\n",
    "        \n",
    "        # Test files - sample 2,000 from each  \n",
    "        test_files = [\n",
    "            ('original.pkl', 'original'),\n",
    "            ('rotation_15.pkl', 'rotation_15'),\n",
    "            ('noise.pkl', 'noise'),\n",
    "            ('scaling_0.8.pkl', 'scaling_08'),\n",
    "            ('occlusion_25.pkl', 'occlusion_25'),\n",
    "            ('all_combined.pkl', 'all_combined')\n",
    "        ]\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        for dataset in datasets:\n",
    "            self.logger.info(f\"Processing dataset: {dataset.upper()}\")\n",
    "            all_results[dataset] = {'train': {}, 'test': {}}\n",
    "            \n",
    "            # Process TRAIN files (10k samples each)\n",
    "            self.logger.info(\"Processing TRAIN files...\")\n",
    "            for train_file, aug_name in train_files:\n",
    "                train_path = os.path.join(self.processed_dir, f\"{dataset}_train\", train_file)\n",
    "                \n",
    "                if os.path.exists(train_path):\n",
    "                    self.logger.info(f\"Extracting features from TRAIN: {train_file}\")\n",
    "                    results = self.extract_features_for_file(\n",
    "                        train_path, dataset, 'train', aug_name, sample_size=10000\n",
    "                    )\n",
    "                    all_results[dataset]['train'][aug_name] = results\n",
    "                    self.logger.info(f\"Completed TRAIN: {train_file}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"Train file not found: {train_path}\")\n",
    "            \n",
    "            # Process TEST files (2k samples each)\n",
    "            self.logger.info(\"Processing TEST files...\")\n",
    "            for test_file, aug_name in test_files:\n",
    "                test_path = os.path.join(self.processed_dir, f\"{dataset}_test\", test_file)\n",
    "                \n",
    "                if os.path.exists(test_path):\n",
    "                    self.logger.info(f\"Extracting features from TEST: {test_file}\")\n",
    "                    results = self.extract_features_for_file(\n",
    "                        test_path, dataset, 'test', aug_name, sample_size=2000\n",
    "                    )\n",
    "                    all_results[dataset]['test'][aug_name] = results\n",
    "                    self.logger.info(f\"Completed TEST: {test_file}\")\n",
    "                else:\n",
    "                    self.logger.warning(f\"Test file not found: {test_path}\")\n",
    "        \n",
    "        self.logger.info(\"Feature extraction pipeline completed successfully\")\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = os.path.join(self.feature_dir, \"extraction_summary.pkl\")\n",
    "        with open(summary_path, 'wb') as f:\n",
    "            pickle.dump(all_results, f)\n",
    "        \n",
    "        self.logger.info(f\"Extraction summary saved to: {summary_path}\")\n",
    "        return all_results\n",
    "\n",
    "# Run the feature extraction\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Step 3: Feature Extraction\")\n",
    "    print(\"This will extract LBP, HOG, and SIFT features from:\")\n",
    "    print(\"- TRAIN files: original, mixed_augmented, combined_augmented (10k samples each)\")\n",
    "    print(\"- TEST files: original, rotation_15, noise, scaling_0.8, occlusion_25, all_combined (2k samples each)\")\n",
    "    print(\"Features will be saved in: ./data/features/\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    extractor = FeatureExtractor()\n",
    "    results = extractor.run_feature_extraction()\n",
    "    \n",
    "    print(\"Feature extraction completed!\")\n",
    "    print(\"All features saved in: ./data/features/\")\n",
    "    print(\"Check the logs for detailed information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a8bd6",
   "metadata": {},
   "source": [
    "Step 4 (Clustering and BoW using KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b4d21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ProfessionalFeatureProcessor:\n",
    "    def __init__(self, feature_dir='./data/features', output_dir='./data/processed', log_dir='./data/logs'):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.log_dir = log_dir\n",
    "        self._setup_logging()\n",
    "        self._create_directories()\n",
    "        self.config = {\n",
    "            'vocab_sizes': [50, 100, 200],\n",
    "            'max_descriptors': 100000,\n",
    "            'batch_size': 1000,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        self.logger.info(\"Feature processor initialized successfully\")\n",
    "\n",
    "    def _create_directories(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "    def _setup_logging(self):\n",
    "        log_file = os.path.join(self.log_dir, f\"processing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "        logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger()\n",
    "\n",
    "    def load_features(self, dataset='mnist', method='SIFT_BoW', subset='train'):\n",
    "        path = Path(self.feature_dir) / dataset / method / subset\n",
    "        all_features, all_labels = [], []\n",
    "        for file in path.glob('*.pkl'):\n",
    "            with open(file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            all_features.append(data['features'])\n",
    "            all_labels.append(data['labels'])\n",
    "        X = np.vstack(all_features)\n",
    "        y = np.hstack(all_labels)\n",
    "        self.logger.info(f\"Loaded features from {path} with shape {X.shape}\")\n",
    "        return X, y\n",
    "\n",
    "    def perform_clustering(self, X):\n",
    "        clustered_data = {}\n",
    "        for vocab_size in self.config['vocab_sizes']:\n",
    "            self.logger.info(f\"Clustering with vocab size {vocab_size}\")\n",
    "            kmeans = MiniBatchKMeans(n_clusters=vocab_size, batch_size=self.config['batch_size'], random_state=self.config['random_state'])\n",
    "            kmeans.fit(X[:self.config['max_descriptors']])\n",
    "            clustered_data[vocab_size] = kmeans\n",
    "            output_file = os.path.join(self.output_dir, f\"kmeans_{vocab_size}.pkl\")\n",
    "            with open(output_file, 'wb') as f:\n",
    "                pickle.dump(kmeans, f)\n",
    "            self.logger.info(f\"Saved KMeans model for vocab {vocab_size} at {output_file}\")\n",
    "        return clustered_data\n",
    "\n",
    "    def transform_features(self, X, clustered_data):\n",
    "        transformed_data = {}\n",
    "        for vocab_size, kmeans in clustered_data.items():\n",
    "            labels = kmeans.predict(X)\n",
    "            hist = np.zeros((X.shape[0], vocab_size), dtype=np.float32)\n",
    "            for i, label in enumerate(labels):\n",
    "                hist[i, label] += 1\n",
    "            scaler = StandardScaler()\n",
    "            hist = scaler.fit_transform(hist)\n",
    "            transformed_data[vocab_size] = hist\n",
    "            output_file = os.path.join(self.output_dir, f\"transformed_{vocab_size}.pkl\")\n",
    "            with open(output_file, 'wb') as f:\n",
    "                pickle.dump(hist, f)\n",
    "            self.logger.info(f\"Saved transformed features for vocab {vocab_size} at {output_file}\")\n",
    "        return transformed_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    processor = ProfessionalFeatureProcessor()\n",
    "    X, y = processor.load_features(dataset='mnist', method='SIFT_BoW', subset='train')\n",
    "    clustered = processor.perform_clustering(X)\n",
    "    transformed = processor.transform_features(X, clustered)\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e680b",
   "metadata": {},
   "source": [
    "Model Training and Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b473a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class SIFTBowClassification:\n",
    "    def __init__(self):\n",
    "        self.base_path = \"data/features/mnist/SIFT_BoW\"\n",
    "        self.results_path = \"mnist_results/SIFT_BoW\"\n",
    "        os.makedirs(self.results_path, exist_ok=True)\n",
    "        \n",
    "        # Define training and test files\n",
    "        self.train_files = [\n",
    "            'original.pkl',\n",
    "            'combined_augmented.pkl', \n",
    "            'mixed_augmented.pkl'\n",
    "        ]\n",
    "        \n",
    "        self.test_files = [\n",
    "            'original.pkl',\n",
    "            'noise.pkl',\n",
    "            'occlusion_25.pkl',\n",
    "            'rotation_15.pkl', \n",
    "            'scaling_08.pkl',\n",
    "            'all_combined.pkl'\n",
    "        ]\n",
    "        \n",
    "        # Initialize classifiers\n",
    "        self.classifiers = {\n",
    "            'SVM_RBF': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Random_Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "            'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000),\n",
    "            'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "        }\n",
    "        \n",
    "        # Store all results for top model selection\n",
    "        self.all_results = []\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('sift_bow_classification.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def debug_data_structure(self, data, file_path):\n",
    "        \"\"\"Debug function to understand data structure\"\"\"\n",
    "        logging.info(f\"DEBUG: Data type: {type(data)}\")\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            logging.info(f\"DEBUG: Dictionary keys: {list(data.keys())}\")\n",
    "            for key, value in data.items():\n",
    "                if hasattr(value, 'shape'):\n",
    "                    logging.info(f\"DEBUG: Key '{key}' shape: {value.shape}\")\n",
    "                elif hasattr(value, '__len__'):\n",
    "                    logging.info(f\"DEBUG: Key '{key}' length: {len(value)}\")\n",
    "                else:\n",
    "                    logging.info(f\"DEBUG: Key '{key}' type: {type(value)}\")\n",
    "        \n",
    "        elif isinstance(data, (list, tuple)):\n",
    "            logging.info(f\"DEBUG: List/tuple length: {len(data)}\")\n",
    "            for i, item in enumerate(data):\n",
    "                if hasattr(item, 'shape'):\n",
    "                    logging.info(f\"DEBUG: Item {i} shape: {item.shape}\")\n",
    "                elif hasattr(item, '__len__'):\n",
    "                    logging.info(f\"DEBUG: Item {i} length: {len(item)}\")\n",
    "                else:\n",
    "                    logging.info(f\"DEBUG: Item {i} type: {type(item)}\")\n",
    "        \n",
    "        else:\n",
    "            logging.info(f\"DEBUG: Unknown data structure in {file_path}\")\n",
    "\n",
    "    def extract_features_and_labels(self, data, file_path):\n",
    "        \"\"\"Extract features and labels from various data formats\"\"\"\n",
    "        features = None\n",
    "        labels = None\n",
    "        \n",
    "        # Try different data structures\n",
    "        if isinstance(data, dict):\n",
    "            # Try common keys for features\n",
    "            feature_keys = ['features', 'descriptors', 'bow_features', 'histograms', \n",
    "                          'sift_bow_features', 'sift_features', 'bow_vectors']\n",
    "            for key in feature_keys:\n",
    "                if key in data:\n",
    "                    features = data[key]\n",
    "                    logging.info(f\"Found features with key: '{key}'\")\n",
    "                    break\n",
    "            \n",
    "            # Try common keys for labels\n",
    "            label_keys = ['labels', 'targets', 'target', 'y']\n",
    "            for key in label_keys:\n",
    "                if key in data:\n",
    "                    labels = data[key]\n",
    "                    logging.info(f\"Found labels with key: '{key}'\")\n",
    "                    break\n",
    "            \n",
    "            # If still not found, try to extract first two elements\n",
    "            if features is None and len(data) >= 2:\n",
    "                keys = list(data.keys())\n",
    "                features = data.get(keys[0])\n",
    "                labels = data.get(keys[1])\n",
    "                logging.info(f\"Using first two keys: '{keys[0]}' and '{keys[1]}'\")\n",
    "                \n",
    "        elif isinstance(data, (list, tuple)):\n",
    "            if len(data) >= 2:\n",
    "                features = data[0]\n",
    "                labels = data[1]\n",
    "                logging.info(\"Using first two elements of list/tuple\")\n",
    "        \n",
    "        # Handle sparse matrices\n",
    "        if features is not None and hasattr(features, 'toarray'):\n",
    "            features = features.toarray()\n",
    "            logging.info(\"Converted sparse matrix to dense array\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        if features is not None:\n",
    "            features = np.array(features)\n",
    "        if labels is not None:\n",
    "            labels = np.array(labels)\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def load_features(self, file_path, n_samples=None):\n",
    "        \"\"\"Load features from pickle file with extensive error handling\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logging.error(f\"File not found: {file_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        try:\n",
    "            logging.info(f\"Loading features from: {file_path}\")\n",
    "            \n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # Debug the data structure\n",
    "            self.debug_data_structure(data, file_path)\n",
    "            \n",
    "            # Extract features and labels\n",
    "            features, labels = self.extract_features_and_labels(data, file_path)\n",
    "            \n",
    "            if features is None or labels is None:\n",
    "                logging.error(f\"Could not extract features and labels from {file_path}\")\n",
    "                return None, None\n",
    "            \n",
    "            logging.info(f\"Raw features shape: {features.shape}, labels shape: {labels.shape}\")\n",
    "            \n",
    "            # Clean features (handle NaN/inf)\n",
    "            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # Reshape if needed (for 2D+ features)\n",
    "            if len(features.shape) > 2:\n",
    "                original_shape = features.shape\n",
    "                features = features.reshape(features.shape[0], -1)\n",
    "                logging.info(f\"Reshaped features from {original_shape} to {features.shape}\")\n",
    "            \n",
    "            # Ensure we have at least 2D features\n",
    "            if len(features.shape) == 1:\n",
    "                features = features.reshape(-1, 1)\n",
    "                logging.info(f\"Reshaped 1D features to 2D: {features.shape}\")\n",
    "            \n",
    "            # Sample if requested\n",
    "            if n_samples and len(features) > n_samples:\n",
    "                logging.info(f\"Sampling {n_samples} from {len(features)} total samples\")\n",
    "                # Use stratified sampling to maintain class distribution\n",
    "                splitter = StratifiedShuffleSplit(n_splits=1, train_size=n_samples, random_state=42)\n",
    "                for train_idx, _ in splitter.split(features, labels):\n",
    "                    features = features[train_idx]\n",
    "                    labels = labels[train_idx]\n",
    "                    break\n",
    "            \n",
    "            logging.info(f\"Final features shape: {features.shape}, labels shape: {labels.shape}\")\n",
    "            return features, labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "            import traceback\n",
    "            logging.error(traceback.format_exc())\n",
    "            return None, None\n",
    "    \n",
    "    def validate_data(self, features, labels, file_path):\n",
    "        \"\"\"Validate the loaded data\"\"\"\n",
    "        if features is None or labels is None:\n",
    "            return False\n",
    "        \n",
    "        if len(features) != len(labels):\n",
    "            logging.error(f\"Feature count ({len(features)}) != label count ({len(labels)}) in {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            logging.error(f\"No features loaded from {file_path}\")\n",
    "            return False\n",
    "        \n",
    "        # Check for constant features\n",
    "        if features.shape[1] > 1:\n",
    "            std_dev = np.std(features, axis=0)\n",
    "            constant_features = np.sum(std_dev == 0)\n",
    "            if constant_features > 0:\n",
    "                logging.warning(f\"Found {constant_features} constant features in {file_path}\")\n",
    "        \n",
    "        logging.info(f\"Data validation passed for {file_path}\")\n",
    "        return True\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, y_train, X_test, y_test, train_file, test_file):\n",
    "        \"\"\"Train all classifiers and evaluate on test data\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for clf_name, classifier in self.classifiers.items():\n",
    "            try:\n",
    "                logging.info(f\"Training {clf_name} on {train_file} -> {test_file}\")\n",
    "                \n",
    "                # Train classifier\n",
    "                classifier.fit(X_train, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = classifier.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "                \n",
    "                # Generate classification report\n",
    "                report = classification_report(y_test, y_pred, digits=4, output_dict=True)\n",
    "                \n",
    "                results[clf_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_macro': f1_macro,\n",
    "                    'classification_report': report,\n",
    "                    'predictions': y_pred,\n",
    "                    'model': classifier\n",
    "                }\n",
    "                \n",
    "                # Store for top model selection\n",
    "                self.all_results.append({\n",
    "                    'train_file': train_file,\n",
    "                    'test_file': test_file,\n",
    "                    'classifier': clf_name,\n",
    "                    'f1_macro': f1_macro,\n",
    "                    'accuracy': accuracy,\n",
    "                    'model': classifier\n",
    "                })\n",
    "                \n",
    "                logging.info(f\"{clf_name} - Accuracy: {accuracy:.4f}, F1 Macro: {f1_macro:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error training {clf_name}: {str(e)}\")\n",
    "                import traceback\n",
    "                logging.error(traceback.format_exc())\n",
    "                results[clf_name] = None\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_all_experiments(self):\n",
    "        \"\"\"Run all 18 experiments for SIFT_BoW\"\"\"\n",
    "        logging.info(\"Starting SIFT_BoW Classification Experiments\")\n",
    "        logging.info(\"18 experiments: 3 train files Ã— 6 test files\")\n",
    "        \n",
    "        # Check if base directory exists\n",
    "        if not os.path.exists(self.base_path):\n",
    "            logging.error(f\"SIFT_BoW base directory not found: {self.base_path}\")\n",
    "            return\n",
    "        \n",
    "        # Check train and test directories\n",
    "        train_dir = os.path.join(self.base_path, \"train\")\n",
    "        test_dir = os.path.join(self.base_path, \"test\")\n",
    "        \n",
    "        if not os.path.exists(train_dir):\n",
    "            logging.error(f\"Train directory not found: {train_dir}\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(test_dir):\n",
    "            logging.error(f\"Test directory not found: {test_dir}\")\n",
    "            return\n",
    "        \n",
    "        # Iterate through all train-test combinations\n",
    "        for train_file in self.train_files:\n",
    "            for test_file in self.test_files:\n",
    "                combo_name = f\"{train_file.replace('.pkl', '')}_{test_file.replace('.pkl', '')}\"\n",
    "                logging.info(f\"\\n{'='*50}\")\n",
    "                logging.info(f\"Processing combination: {combo_name}\")\n",
    "                logging.info(f\"{'='*50}\")\n",
    "                \n",
    "                # Load training data\n",
    "                train_path = os.path.join(train_dir, train_file)\n",
    "                X_train, y_train = self.load_features(train_path, 1000)\n",
    "                \n",
    "                if not self.validate_data(X_train, y_train, train_path):\n",
    "                    logging.error(f\"Failed to load/validate training data: {train_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load test data\n",
    "                test_path = os.path.join(test_dir, test_file)\n",
    "                X_test, y_test = self.load_features(test_path, 200)\n",
    "                \n",
    "                if not self.validate_data(X_test, y_test, test_path):\n",
    "                    logging.error(f\"Failed to load/validate test data: {test_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Train and evaluate\n",
    "                results = self.train_and_evaluate(X_train, y_train, X_test, y_test, train_file, test_file)\n",
    "                \n",
    "                # Save individual result file\n",
    "                self.save_individual_result(combo_name, results, train_file, test_file, y_test)\n",
    "        \n",
    "        # Save summary and select top models\n",
    "        if self.all_results:\n",
    "            self.save_summary_and_top_models()\n",
    "            logging.info(\"SIFT_BoW experiments completed successfully!\")\n",
    "        else:\n",
    "            logging.error(\"No experiments were completed successfully!\")\n",
    "    \n",
    "    def save_individual_result(self, combo_name, results, train_file, test_file, y_test):\n",
    "        \"\"\"Save individual experiment result to text file\"\"\"\n",
    "        filename = f\"{combo_name}.txt\"\n",
    "        filepath = os.path.join(self.results_path, filename)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"SIFT_BoW MNIST Classification Result\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Training File: {train_file}\\n\")\n",
    "            f.write(f\"Test File: {test_file}\\n\")\n",
    "            f.write(f\"Training Samples: 1000\\n\")\n",
    "            f.write(f\"Test Samples: 200\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            successful_models = 0\n",
    "            for clf_name, result in results.items():\n",
    "                if result is None:\n",
    "                    f.write(f\"Classifier: {clf_name}\\n\")\n",
    "                    f.write(\"Status: Failed to train\\n\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "                    continue\n",
    "                \n",
    "                successful_models += 1\n",
    "                f.write(f\"Classifier: {clf_name}\\n\")\n",
    "                f.write(f\"Accuracy: {result['accuracy']:.4f}\\n\")\n",
    "                f.write(f\"F1 Macro: {result['f1_macro']:.4f}\\n\\n\")\n",
    "                \n",
    "                # Convert dict report to string\n",
    "                report_str = classification_report(y_test, result['predictions'], digits=4)\n",
    "                f.write(\"Classification Report:\\n\")\n",
    "                f.write(report_str)\n",
    "                f.write(\"\\n\" + \"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Successful models: {successful_models}/{len(self.classifiers)}\\n\")\n",
    "            \n",
    "            # --- Top 3 models for this iteration (by F1 Macro) ---\n",
    "            try:\n",
    "                entries = []\n",
    "                for clf_name, res in results.items():\n",
    "                    if res is None: \n",
    "                        continue\n",
    "                    entries.append((res.get('f1_macro', 0.0), res.get('accuracy', 0.0), clf_name))\n",
    "                entries.sort(key=lambda x: x[0], reverse=True)\n",
    "                f.write(\"\\nTOP 3 MODELS (this iteration)\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                for rank, (f1, acc, clf) in enumerate(entries[:3], start=1):\n",
    "                    f.write(f\"{rank}. {clf:<20}  F1: {f1:.4f}  Acc: {acc:.4f}\\n\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Top-3 write failed for {combo_name}: {str(e)}\")\n",
    "        \n",
    "        logging.info(f\"Saved result file: {filename}\")\n",
    "    \n",
    "    def save_summary_and_top_models(self):\n",
    "        \"\"\"Save summary of all experiments and select top 3 models\"\"\"\n",
    "        summary_file = os.path.join(self.results_path, \"SIFT_BoW_Summary.txt\")\n",
    "        \n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"SIFT_BoW MNIST Classification - Complete Summary\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Experiments: {len(self.all_results)}\\n\")\n",
    "            f.write(f\"Training samples per file: 1000\\n\")\n",
    "            f.write(f\"Test samples per file: 200\\n\")\n",
    "            f.write(f\"Primary metric: F1 Macro\\n\\n\")\n",
    "            \n",
    "            # Sort all results by F1 macro descending\n",
    "            sorted_results = sorted(self.all_results, key=lambda x: x['f1_macro'], reverse=True)\n",
    "            \n",
    "            # Top 3 models section\n",
    "            f.write(\"TOP 3 MODELS (by F1 Macro)\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            for i, result in enumerate(sorted_results[:3]):\n",
    "                f.write(f\"Rank {i+1}:\\n\")\n",
    "                f.write(f\"  Training File: {result['train_file']}\\n\")\n",
    "                f.write(f\"  Test File: {result['test_file']}\\n\")\n",
    "                f.write(f\"  Classifier: {result['classifier']}\\n\")\n",
    "                f.write(f\"  F1 Macro: {result['f1_macro']:.4f}\\n\")\n",
    "                f.write(f\"  Accuracy: {result['accuracy']:.4f}\\n\\n\")\n",
    "            \n",
    "            # Detailed results matrix\n",
    "            f.write(\"\\nF1 MACRO SCORES MATRIX\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            # Header row\n",
    "            f.write(f\"{'Train/Test':<25}\")\n",
    "            for test_file in self.test_files:\n",
    "                f.write(f\"{test_file.replace('.pkl', ''):<15}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"-\" * 130 + \"\\n\")\n",
    "            \n",
    "            # For each classifier, create a matrix\n",
    "            for clf_name in self.classifiers.keys():\n",
    "                f.write(f\"\\nClassifier: {clf_name}\\n\")\n",
    "                f.write(\"-\" * 130 + \"\\n\")\n",
    "                \n",
    "                for train_file in self.train_files:\n",
    "                    f.write(f\"{train_file.replace('.pkl', ''):<25}\")\n",
    "                    \n",
    "                    for test_file in self.test_files:\n",
    "                        # Find the result for this combination\n",
    "                        combo_result = next((r for r in self.all_results if \n",
    "                                           r['train_file'] == train_file and \n",
    "                                           r['test_file'] == test_file and \n",
    "                                           r['classifier'] == clf_name), None)\n",
    "                        \n",
    "                        if combo_result:\n",
    "                            f.write(f\"{combo_result['f1_macro']:<15.4f}\")\n",
    "                        else:\n",
    "                            f.write(f\"{'N/A':<15}\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Save top 3 models to pickle files\n",
    "            f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "            f.write(\"TOP 3 MODELS SAVED\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            # Create top_models directory\n",
    "            top_models_dir = os.path.join(self.results_path, \"top_models\")\n",
    "            os.makedirs(top_models_dir, exist_ok=True)\n",
    "            \n",
    "            for i, result in enumerate(sorted_results[:3]):\n",
    "                model_filename = f\"top_model_{i+1}_{result['classifier']}.pkl\"\n",
    "                model_path = os.path.join(top_models_dir, model_filename)\n",
    "                \n",
    "                # Save the model\n",
    "                with open(model_path, 'wb') as model_file:\n",
    "                    pickle.dump({\n",
    "                        'model': result['model'],\n",
    "                        'train_file': result['train_file'],\n",
    "                        'test_file': result['test_file'],\n",
    "                        'classifier': result['classifier'],\n",
    "                        'f1_macro': result['f1_macro'],\n",
    "                        'accuracy': result['accuracy'],\n",
    "                        'rank': i+1\n",
    "                    }, model_file)\n",
    "                \n",
    "                f.write(f\"Model {i+1}: {model_filename}\\n\")\n",
    "                f.write(f\"  - Training: {result['train_file']}\\n\")\n",
    "                f.write(f\"  - Testing: {result['test_file']}\\n\")\n",
    "                f.write(f\"  - Classifier: {result['classifier']}\\n\")\n",
    "                f.write(f\"  - F1 Macro: {result['f1_macro']:.4f}\\n\")\n",
    "                f.write(f\"  - Saved to: {model_path}\\n\\n\")\n",
    "        \n",
    "        logging.info(f\"Summary saved to: {summary_file}\")\n",
    "        logging.info(\"Top 3 models selected and saved!\")\n",
    "\n",
    "class MNISTClassificationExperiment:\n",
    "    def __init__(self):\n",
    "        self.base_path = \"data/features/mnist\"\n",
    "        self.results_path = \"mnist_results\"\n",
    "        os.makedirs(self.results_path, exist_ok=True)\n",
    "        \n",
    "        # Define training and test files\n",
    "        self.train_files = [\n",
    "            'original.pkl',\n",
    "            'combined_augmented.pkl', \n",
    "            'mixed_augmented.pkl'\n",
    "        ]\n",
    "        \n",
    "        self.test_files = [\n",
    "            'original.pkl',\n",
    "            'noise.pkl',\n",
    "            'occlusion_25.pkl',\n",
    "            'rotation_15.pkl', \n",
    "            'scaling_08.pkl',\n",
    "            'all_combined.pkl'\n",
    "        ]\n",
    "        \n",
    "        # Initialize classifiers\n",
    "        self.classifiers = {\n",
    "            'SVM_RBF': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "            'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Random_Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "            'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "            'MLP': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=1000),\n",
    "            'LightGBM': lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "        }\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler('mnist_classification.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def load_features(self, file_path, n_samples=None):\n",
    "        \"\"\"Load features from pickle file with robust error handling\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            logging.warning(f\"File not found: {file_path}\")\n",
    "            return None, None\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            \n",
    "            # Handle different data formats\n",
    "            if isinstance(data, dict):\n",
    "                features = data.get('features', data.get('descriptors', None))\n",
    "                labels = data.get('labels', None)\n",
    "            elif isinstance(data, (list, tuple)) and len(data) >= 2:\n",
    "                features, labels = data[0], data[1]\n",
    "            else:\n",
    "                logging.error(f\"Unexpected data format in {file_path}\")\n",
    "                return None, None\n",
    "            \n",
    "            if features is None or labels is None:\n",
    "                logging.error(f\"Could not extract features and labels from {file_path}\")\n",
    "                return None, None\n",
    "            \n",
    "            features = np.array(features)\n",
    "            labels = np.array(labels)\n",
    "            \n",
    "            # Clean features\n",
    "            features = np.nan_to_num(features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            # Sample if requested\n",
    "            if n_samples and len(features) > n_samples:\n",
    "                # Use stratified sampling to maintain class distribution\n",
    "                splitter = StratifiedShuffleSplit(n_splits=1, train_size=n_samples, random_state=42)\n",
    "                for train_idx, _ in splitter.split(features, labels):\n",
    "                    features = features[train_idx]\n",
    "                    labels = labels[train_idx]\n",
    "                    break\n",
    "            \n",
    "            return features, labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {file_path}: {str(e)}\")\n",
    "            return None, None\n",
    "    \n",
    "    def train_and_evaluate(self, X_train, y_train, X_test, y_test, train_file, test_file, feature_type):\n",
    "        \"\"\"Train all classifiers and evaluate on test data\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for clf_name, classifier in self.classifiers.items():\n",
    "            try:\n",
    "                logging.info(f\"Training {clf_name} on {train_file} -> {test_file}\")\n",
    "                \n",
    "                # Train classifier\n",
    "                classifier.fit(X_train, y_train)\n",
    "                \n",
    "                # Predictions\n",
    "                y_pred = classifier.predict(X_test)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "                \n",
    "                # Generate classification report\n",
    "                report = classification_report(y_test, y_pred, digits=4)\n",
    "                \n",
    "                results[clf_name] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'f1_macro': f1_macro,\n",
    "                    'classification_report': report,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error training {clf_name}: {str(e)}\")\n",
    "                results[clf_name] = None\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_experiment_for_feature_type(self, feature_type):\n",
    "        \"\"\"Run all 18 experiments for a specific feature type\"\"\"\n",
    "        logging.info(f\"Starting experiments for {feature_type}\")\n",
    "        \n",
    "        # Create feature type directory\n",
    "        feature_dir = os.path.join(self.results_path, feature_type)\n",
    "        os.makedirs(feature_dir, exist_ok=True)\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Iterate through all train-test combinations\n",
    "        for train_file in self.train_files:\n",
    "            for test_file in self.test_files:\n",
    "                combo_name = f\"{train_file.replace('.pkl', '')}_{test_file.replace('.pkl', '')}\"\n",
    "                logging.info(f\"Processing combination: {combo_name}\")\n",
    "                \n",
    "                # Load training data\n",
    "                train_path = os.path.join(self.base_path, feature_type, \"train\", train_file)\n",
    "                X_train, y_train = self.load_features(train_path, 1000)\n",
    "                \n",
    "                if X_train is None:\n",
    "                    logging.error(f\"Failed to load training data: {train_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Load test data\n",
    "                test_path = os.path.join(self.base_path, feature_type, \"test\", test_file)\n",
    "                X_test, y_test = self.load_features(test_path, 200)\n",
    "                \n",
    "                if X_test is None:\n",
    "                    logging.error(f\"Failed to load test data: {test_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Reshape if needed\n",
    "                if len(X_train.shape) > 2:\n",
    "                    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "                    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "                \n",
    "                # Train and evaluate\n",
    "                results = self.train_and_evaluate(X_train, y_train, X_test, y_test, train_file, test_file, feature_type)\n",
    "                all_results[combo_name] = results\n",
    "                \n",
    "                # Save individual result file\n",
    "                self.save_individual_result(feature_dir, combo_name, results, train_file, test_file, feature_type)\n",
    "        \n",
    "        # Save summary for this feature type\n",
    "        self.save_feature_summary(feature_dir, feature_type, all_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def save_individual_result(self, feature_dir, combo_name, results, train_file, test_file, feature_type):\n",
    "        \"\"\"Save individual experiment result to text file\"\"\"\n",
    "        filename = f\"{combo_name}.txt\"\n",
    "        filepath = os.path.join(feature_dir, filename)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            f.write(\"MNIST Classification Result\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Feature Type: {feature_type}\\n\")\n",
    "            f.write(f\"Training File: {train_file}\\n\")\n",
    "            f.write(f\"Test File: {test_file}\\n\")\n",
    "            f.write(f\"Training Samples: 1000\\n\")\n",
    "            f.write(f\"Test Samples: 200\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            \n",
    "            for clf_name, result in results.items():\n",
    "                if result is None:\n",
    "                    f.write(f\"Classifier: {clf_name}\\n\")\n",
    "                    f.write(\"Status: Failed to train\\n\\n\")\n",
    "                    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "                    continue\n",
    "                \n",
    "                f.write(f\"Classifier: {clf_name}\\n\")\n",
    "                f.write(f\"Accuracy: {result['accuracy']:.4f}\\n\")\n",
    "                f.write(f\"F1 Macro: {result['f1_macro']:.4f}\\n\\n\")\n",
    "                f.write(\"Classification Report:\\n\")\n",
    "                f.write(result['classification_report'])\n",
    "                f.write(\"\\n\" + \"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            # --- Top 3 models for this iteration (by F1 Macro) ---\n",
    "            try:\n",
    "                entries = []\n",
    "                for clf_name, res in results.items():\n",
    "                    if res is None: \n",
    "                        continue\n",
    "                    entries.append((res.get('f1_macro', 0.0), res.get('accuracy', 0.0), clf_name))\n",
    "                entries.sort(key=lambda x: x[0], reverse=True)\n",
    "                f.write(\"\\nTOP 3 MODELS (this iteration)\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                for rank, (f1, acc, clf) in enumerate(entries[:3], start=1):\n",
    "                    f.write(f\"{rank}. {clf:<20}  F1: {f1:.4f}  Acc: {acc:.4f}\\n\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Top-3 write failed for {combo_name}: {str(e)}\")\n",
    "    \n",
    "    def save_feature_summary(self, feature_dir, feature_type, all_results):\n",
    "        \"\"\"Save summary of all experiments for a feature type\"\"\"\n",
    "        summary_file = os.path.join(feature_dir, f\"summary_{feature_type}.txt\")\n",
    "        \n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(f\"MNIST {feature_type} Classification - Complete Summary\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(f\"Total Experiments: {len(all_results)} (3 train files Ã— 6 test files)\\n\")\n",
    "            f.write(f\"Training samples per file: 1000\\n\")\n",
    "            f.write(f\"Test samples per file: 200\\n\")\n",
    "            f.write(f\"Primary metric: F1 Macro\\n\\n\")\n",
    "            \n",
    "            # Create a matrix of F1 scores\n",
    "            f.write(\"F1 Macro Scores Matrix\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            # Header row\n",
    "            f.write(f\"{'Train/Test':<25}\")\n",
    "            for test_file in self.test_files:\n",
    "                f.write(f\"{test_file.replace('.pkl', ''):<15}\")\n",
    "            f.write(\"\\n\")\n",
    "            f.write(\"-\" * 130 + \"\\n\")\n",
    "            \n",
    "            # For each classifier, create a matrix\n",
    "            for clf_name in self.classifiers.keys():\n",
    "                f.write(f\"\\nClassifier: {clf_name}\\n\")\n",
    "                f.write(\"-\" * 130 + \"\\n\")\n",
    "                \n",
    "                for train_file in self.train_files:\n",
    "                    f.write(f\"{train_file.replace('.pkl', ''):<25}\")\n",
    "                    \n",
    "                    for test_file in self.test_files:\n",
    "                        combo_name = f\"{train_file.replace('.pkl', '')}_{test_file.replace('.pkl', '')}\"\n",
    "                        \n",
    "                        if combo_name in all_results and all_results[combo_name][clf_name] is not None:\n",
    "                            f1_score = all_results[combo_name][clf_name]['f1_macro']\n",
    "                            f.write(f\"{f1_score:<15.4f}\")\n",
    "                        else:\n",
    "                            f.write(f\"{'FAILED':<15}\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            # Best performing combinations\n",
    "            f.write(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "            f.write(\"TOP PERFORMING COMBINATIONS (by F1 Macro)\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            all_scores = []\n",
    "            for combo_name, combo_results in all_results.items():\n",
    "                for clf_name, result in combo_results.items():\n",
    "                    if result is not None:\n",
    "                        all_scores.append({\n",
    "                            'combination': combo_name,\n",
    "                            'classifier': clf_name,\n",
    "                            'f1_macro': result['f1_macro'],\n",
    "                            'accuracy': result['accuracy']\n",
    "                        })\n",
    "            \n",
    "            # Sort by F1 macro descending\n",
    "            all_scores.sort(key=lambda x: x['f1_macro'], reverse=True)\n",
    "            \n",
    "            f.write(f\"{'Rank':<5} {'Combination':<30} {'Classifier':<20} {'F1 Macro':<12} {'Accuracy':<12}\\n\")\n",
    "            f.write(\"-\" * 80 + \"\\n\")\n",
    "            \n",
    "            for i, score in enumerate(all_scores[:20]):  # Top 20\n",
    "                f.write(f\"{i+1:<5} {score['combination']:<30} {score['classifier']:<20} \"\n",
    "                       f\"{score['f1_macro']:<12.4f} {score['accuracy']:<12.4f}\\n\")\n",
    "    \n",
    "    def run_complete_experiment(self):\n",
    "        \"\"\"Run complete MNIST classification experiment for all feature types\"\"\"\n",
    "        logging.info(\"Starting Complete MNIST Classification Experiment\")\n",
    "        logging.info(\"This will generate 18 classification reports for each feature type\")\n",
    "        logging.info(\"Total experiments: 3 feature types Ã— 3 train files Ã— 6 test files = 54 combinations\")\n",
    "        \n",
    "        feature_types = ['HOG', 'LBP', 'SIFT_BoW']\n",
    "        \n",
    "        for feature_type in feature_types:\n",
    "            logging.info(f\"\\n{'='*60}\")\n",
    "            logging.info(f\"Processing Feature Type: {feature_type}\")\n",
    "            logging.info(f\"{'='*60}\")\n",
    "\n",
    "            # If SIFT_BoW, use the specialized SIFTBowClassification integrated above\n",
    "            if feature_type == 'SIFT_BoW':\n",
    "                logging.info(\"Detected SIFT_BoW - delegating to integrated SIFTBowClassification module\")\n",
    "                sift_runner = SIFTBowClassification()\n",
    "                sift_runner.run_all_experiments()\n",
    "                continue\n",
    "\n",
    "            # Check if feature directory exists\n",
    "            feature_path = os.path.join(self.base_path, feature_type)\n",
    "            if not os.path.exists(feature_path):\n",
    "                logging.error(f\"Feature directory not found: {feature_path}\")\n",
    "                continue\n",
    "\n",
    "            # Run experiments using existing flow (HOG, LBP)\n",
    "            self.run_experiment_for_feature_type(feature_type)\n",
    "\n",
    "        logging.info(\"EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
    "        logging.info(f\"Results saved to: {self.results_path}\")\n",
    "        logging.info(\"Each feature type folder contains 18 individual result files and 1 summary file\")\n",
    "        logging.info(f\"{'='*60}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    experiment = MNISTClassificationExperiment()\n",
    "    experiment.run_complete_experiment()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
